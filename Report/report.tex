\documentclass[11pt, a4paper, oneside,titlepage]{article}
\usepackage{times}
\usepackage{graphicx}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{epstopdf}
\usepackage{url}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{fixltx2e}
\usepackage[top=1.5in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{parskip}
\usepackage{framed}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{framed}
\usepackage{caption}
\usepackage{listings}

% Proof Style

\renewcommand\abstractname{\textit{Abstract}}
\newcommand{\HRule}{\rule{\linewidth}{0.4mm}}

\renewcommand{\familydefault}{\rmdefault}
\renewcommand{\rmdefault}{cmr}
\lstdefinestyle{customc}{
  breaklines = true,
  frame=single,
  xleftmargin=\parindent
}

\normalfont





\begin{document}
\begin{titlepage}
\begin{center}
	\textsc{\LARGE Imperial College London}\\[1.5cm]

	\textsc{\Large Department of Computing}\\[0.5cm]

	\HRule \\[0.4cm]
	{\LARGE \bfseries On-the-fly Modelling and Prediction of Epidemic Phenomena \\[0.4cm] }
	\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
James \textsc{Hay}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Dr.~William \textsc{Knottenbelt}
\end{flushright}
\end{minipage}
\vfill
\includegraphics[width=5cm]{ImperialCrest.jpg}
\vfill
Submitted in partial fulfilment of the requirements for the MSc Degree in Computing Science of Imperial College London
\vfill
% Bottom of the page
{\large September 2014}
\end{center}
\end{titlepage}

\section*{Abstract}


\newpage
\section*{Acknowledgements}
I would like to thank all my mates who without my mates I would not be
able to do my project, like at all.

\newpage
\section{Introduction}
\subsection{Overview}
This project provides a framework with which the dynamics of
an epidemic event composed of multiple, overlapping sub-epidemics may be modelled and
forecasted in real time. We aim to provide an optimised model fit to a
given set of epidemic data where all model parameters are assumed to
be unknown. The challenge of considering various candidate model
types is also considered. Unlike previous approaches, the presented framework is
implemented in object oriented style using a general purpose
programming language, allowing improved customisability and
speed. Furthermore, we attempt to implement a maximum likelihood based
fitting procedure, which has previously only been considered for
single epidemic models.


\subsection{Motivation}
Epidemic spreading processes can be observed in a wide range of
fields. Any type of interaction between individuals will allow the
propogation of ideas or parasites through a population, with some spreading
processes arising unexpectedly in excess of baackground levels. In the case of
infectious diseases, such outbreaks are termed epidemics. Indeed, many significant
historical events have been heavily influenced by epidemics, and the
WHO estimates that infectious diseases account for more than 13
million deaths per year.\cite{who} It is therefore no wonder that
epidemiology, the study of the mechanisms and population dynamics of
infectious diseases, has become a central field of research. With the
advancement of computing technology and methodology, new opportunities
to develop complex mathematical and computational models of real-time
epidemics have emerged.

Although the study of infectious disease spread continues to be an
area of particular research focus, globalisation and advances in
communication technologies have led to a new and rapidly developing
type of internet based epidemic. With the entire world connected
online, new ideas, trends and information can disseminate through the
world wide web almost instantaneously, and as these spreading
processes become increasingly central to modern day life, interest
from academic, commercial and social fields continues to increase. One
highly relevant theory is that of `memes' as proposed by Richard
Dawkins, who suggests that ideas, behaviours and styles spread like
"mind viruses" between individuals within a culture.\cite{dawkins} The
adaptation of this term to describe the spread of fads on the internet
demonstrates the relevance of studying the dynamics of epidemic
processes over the internet. \cite{meme}

Research into the dynamics of internet-based phenomena are largely at
an early stage, and have mostly focused on Online Social Networks
(OSNs) such as Facebook and Twitter, and content sharing websites such
as YouTube.\cite{hartmann, bieber} The analogy between infectious
diseases and the spread of content online is easy to consider: the
social networks formed on OSNs simulate physical interactions in real
life as users interact and share content on their profiles, whilst
viewing and subsequently sharing a YouTube video may be compared to
contracting and spreading an infectious disease. There is a growing
body of research that aims to use ideas from epidemiology to better
understand the spread of internet-based phenomena. \cite{marily2013,
  meme, wang} Taking inspiration from epidemiology, the study of
internet-based epidemic phenomena has investigated the applicability
of both locally and globally driven models. For example, some studies
have investigated the use of diffusion models to describe the
disemmination of influence, whilst others have investigated the use of
global mathematical models.\cite{marily2013, meme, wang, hu}

Whilst these studies have shown generally promising results, a recent
study highlighted the limitations of the single epidemic based
approach.\cite{marily2013} The co-occurence and interaction between
diseases and with environmental factors is increasingly realised as
important, and the authors suggest that the corresponding field of
\emph{synepidemiology} can be applied to internet-based
epidemics.\cite{singer, bastos} The authors go on to coin the term
\emph{synthedemics} to describe the co-occurence of a set of
infections that may or may not be dependent on each
other.\cite{marily2014} Taking inspiration from Fourier analysis, the
study goes on to investigate how an incoming epidemic signal can be
broken down and described in terms of multiple epidemic components
(Figure 1). Furthermore, the authors build on a previous study to
allow models to be fit in real time without making assumptions
regarding the initial model parameters.




 \begin{figure}[ht!]
\centering
\includegraphics[width=140mm]{marily2014.png}
\caption{Model construction framework as proposed by Nika et al. 2014.\cite{marily2014}}
\label{sir}
\end{figure}



\subsection{Objectives}
The aim of this project is to implement a real time model fitting
framework with which epidemic phenomena might be characterised and
forecasted. When provided with epidemic data up to an arbritary time point,
we attempt to fit an appropriate number of sub-epidemics of various
types to best describe the current data, and to allow future time
points to be predicted. The course of the project can be split up into
the following sub goals:

\subsubsection{Single Epidemics}
The first objective of this project is to implement a single epidemic
fitting framework, treating the growth and recovery rates of the
epidemic as unknown parameters. This framework is then extended to
additionally consider the initial number of susceptible individuals
and the actual epidemic start time as unknown parameters. The model
fitting procedure is undertaken using optimisation with both least
squares and maximum likelihood estimations. The framework allows for
`on the fly' fitting, such that a model may be fit to the data as the
epidemic unfolds and more data points are obtained.

The initial implementation of this framework is initially undetaken using the R
statistics programming environment due to the availability of useful
packages and functions. For example, the \emph{deSolve} for solving
first-order ordinary differential equations (ODEs), the \emph{optim}
function for optimising a set of model parameters, and the
\emph{bbmle} package for maximum likeliihood based fitting.

An extension of this objective that arose during the course of the
project is the implementation of the fitting framework in C++ `from
scratch'.

\subsubsection{Multiple Epidemics}
Not all epidemic phenomena are constrained to a single population, and
the single epidemic fitting methodology is therefore inadequate in
characterising all epidemics. For example, the measure of
\emph{YouTube} video views over time might be composed of multiple
spikes of interest as the video is shared in new online social
groups. This limitation also affects to infectious disease dynamics,
wherein the total number of infected individuals in a country might be
affected by the penetration of the disease into different cities. The
second objective of this project is therefore to implement a model
fitting framework that can simultaneously fit and combine multiple
sub epidemics into a single model. 

As in the single epidemic fitting framework, the multiple epidemic
fitting framework makes no assumptions regarding model parameters such
as growth rate, recovery rate, start time and number of
susceptibles. As above, an initial implementation will be attempted
using R. However, as the computational difficulty of fitting
multiple sets of parameters simultaneously increases as the number of
sub epidemics increases. We therefore provide a final `from scratch' implementation in C++.

A significant extension of this fitting methodology is to allow
different epidemic models to be considered. That is, which one of a
number of model equations can be used to best describe the data? An
object-oriented C++ implementation is therefore provided to consider
the addition and removal of various candidate models to describe a set
of data.

\subsubsection{Maximum Likelihood Estimation}
A novel objective of this project is to use maximum likelihood rather
than least squares to find an optimised model fit. A significant
challenge of this is to implement an efficient likelihood function in
C++ with which a set of parameters might be optimised in reasonable
time. An advanced extension of this objective is to generate confidence
intervals characterise uncertainty in the optimised parameters. 

\subsubsection{Evaluation}
All of the above objectives must be validated, and we use a
number of data sources to assess the model fitting
framework. Synthetic data provides the core of the evaluation, as it
allows for the retrieval of known parameters from artificially
generated data. Finally, we consider the framework's ability to
provide model fits to historic infectious disease data and online
epidemic phenomena.


\subsection{Contributions}


\subsection{Report Structure}

\newpage
\section{Background}
\subsection{Modelling Infectious Disease Dynamics}
Throughout history, infectious diseases can consistently be cited as one of the leading causes of death across the world. Whilst many such diseases may be endemic in a population,  a large proportion  of diseases may outbreak as epidemics. That is, a disease may arise in a community, region or even worldwide in excess of normal levels following a particular outbreak. In an age of increasing urbanisation, global connectivity and a larger immuno-compromised population, monitoring and controlling the spread of epidemics is absolutely paramount.\cite{computational} Recent events such as the 2009 flu pandemic highlight the incredible need for a solid understanding of the underlying mechanisms of such diseases. This section will discuss the history and current standards in epidemiology.

A general understanding of infectious disease behaviour can be seen as early as the 8th century A.D., when the Indians and Chinese used a rudamentary form of vaccination known as variolation to control smallpox.\cite{variolation} Even earlier than this, Hippocrates (c. 460-c. 370 BC) was amongst the first to propose that disease spread could be explained rationally through human behaviour and environmental factors.\cite{hippo} Unfortunately, the understanding of infectious disease dynamics appeared to regress until the 17th century when the collection of the first public health statistics allowed for a more scientific approach. 

One of the first predictive mathematical models was by Bernoulli in 1760, who used mathematical techniques to establish that variolation for smallbox could help increase the life expectancy in the French population.\cite{brauer} Similarly, another systematic study of disease dynamics took place in 1854 by John Snow, who identified a single water pump in London as the likely source of a Cholera epidemic.\cite{snow} However, it was the early 1900s in which the most fundamental advances in mathematical epidemiology were made. Firstly by Ross in 1911, who used a spatial model to describe the spread of malaria due to mosquitoes.\cite{snow} This study was the first to demonstrate that infectious diseases could be controlled by reducing the population of infected individuals below a certain threshold. The next and arguably most central breakthrough was then made by Kermack and McKendrick in 1927, who proposed the use of ordinary differential equations (ODEs).\cite{kermack} ODEs represented the first deterministic, general epidemic model to describe mass action. The general idea behind ODE models in the context of epidemiology is that individuals in a given population are members of various compartments depending on their relationship to the infection (eg. infected, recovered), and individuals switch between compartments as described by these ODEs.

The most basic form of the model prosed by Kermack and McKendrick's ODEs is the Susceptible-Infected-Recovered (SIR) model. Given a population of size N, individuals are divided into three states or compartments: 

\begin{enumerate}
	\item Individuals that are susceptible to the infection, denoted by $S(t)$
	\item Individuals that are infected with the disease and are therefore capable of infecting others, denoted by $I(t)$
	\item Individuals that have been removed from the population or recovered, denoted by $R(t)$. 
\end{enumerate}

Individuals move between compartments in the following order:
\begin{equation*}
S \Longrightarrow I \Longrightarrow R
\end{equation*}
Simply, individuals start off as being free of the disease, but susceptible to infection. Individuals are then infected with the disease and begin to display symptoms, thereby becoming infectious themselves. After a certain period of time, individuals are no longer infectious as they recover and become immune to the disease. In this model, the population size is assumed to be fixed such that:

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{Rplot.png}
\caption{Generic example of the classic SIR model demonstrating the change in population size for each compartment as the epidemic unfolds}
\label{sir}
\end{figure}

\begin{equation*}
N = S(t) + I(t) + R(t)
\end{equation*}

The way in which individuals individuals move between these compartments are described by the following set of ODEs:
\begin{equation*}
	\begin{split}
	&\frac{dS}{dt} = -\beta IS, \\
	&\frac{dI}{dt} = \beta IS - \gamma I, \\
	&\frac{dR}{dt} = \gamma I
	\end{split}
\end{equation*}

The dynamics of these ODEs are influenced by two key parameters: the contact rate, $\beta$, and the recovery rate, $\gamma$. $\beta$ describes the probability of an infected person coming into contact with any susceptible person per unit time, whereas $\gamma$ describes the rate at which an individual recovers from the disease. When $\beta$ is large, the contact rate between individuals is high, and the disease spreads rapidly. Similarly, when $\gamma$ is large, then individuals recover rapidly and move to the recovered compartment quickly. Note that these parameters make global assumptions about the population, in that all individuals have an equal chance of interacting, and all individuals recover at the same rate. Infected individuals therefore come into contact with $\beta$$N$ individuals per unit time. Only susceptible individuals may become infected, and the number of new infections per unit time is therefore $\beta$$N(S/N)$, resulting in a new infection rate of $\beta$$N(S/N)I = $$\beta$$SI$. Finally, as individuals recover with rate $\gamma$, they are removed from the infected compartment and enter the recovered department with rate $\gamma I$.

Considering these parameters allow for useful insights into the dynamics of a given disease: a disease with a high $\beta$ and lower $\gamma$ will obviously spread more than one with a lower contact rate and higher recovery rate. With this in mind, we can make the intuitive leap to conclude that an infection will either: spread as an epidemic when each individual is causing more than one secondary infection; remain endemic in a population when each individual causes exactly one further infection before recovering; will die out when each individual causes less than one secondary infection before recovering. This idea is formalised by the concept of a \emph{basic reproductive number}, $R_0$ (not to be confused with $R(0)$, which denotes the initial size of the recovered population!). $R_0$ denotes the number of secondary infections caused by a single infected individual when introduced into an initial susceptible population, $S(0)$. As this infected individual will come into contact with $\beta N$ individuals per unit time over a period of $1/$$\gamma$ (the mean infectious period), the reproductive number will be given as the number of secondary infections per unit time multiplied by the amount of time that an individual can infect others:\cite{anderson, diekmann}

\begin{equation*}
R_0 = (\beta N)/\gamma
\end{equation*}

$R_0$ describes the number of secondary infections resulting from one individual in a completely susceptible population; however, this rate will obviously decrease as the proportion of susceptible individuals in the population decreases. Rather than considering the initial reproductive number, it is often more useful to consider the effective reproduction number, $R_n$ . In simplest terms, $R_n = R_0 \times s$, where $s$ is the proportion of the population that is susceptible ($S(t)/N$).

As an aside, it should be noted that calculating $R_0$ is a crucial stage in understanding how a disease will spread. A high $R_0$ (eg. malaria) means that the disease will spread rapidly, with each individual causing a high number of secondary infections, whereas a low $R0$ (eg. monkeypox) means that a disease will spread slowly.\cite{vynnycky} As discussed above, an $R_0$ greater than 1 is necessary for an epidemic to take hold. Even at an early stage of an epidemic, $R_0$ can be estimated based on the growth rate of an epidemic, as was the case during the 2003 SARS virus.\cite{sars} Therefore, decreasing the proportion of sucseptible individuals below a certain level (ie. through vaccination) will result in an effective reproduction number of less than 1, preventing the epidemic from taking hold. This critical threshold is defined as the \emph{herd immunity threshold}, and provides a crude but often effective target for immunization programmes:\cite{cockman}

\begin{equation*}
HIT = 1 - \frac{1}{R_0} = \frac{R_0 -1}{R_0}
\end{equation*}

The model shown above describes three compartments, however there are a number of extensions to this model where different compartments and interactions might be appropriate. For example, an "exposed" compartment might be added which encompasses individuals that have been exposed to the disease, but are not yet infectious. Such a model is known as the SEIR model. As well as additional compartments, the transitions between these compartments might be varied. For example, in cases where immunity is only transient, individuals might be able to re-enter the susceptible compartment following recovery (the SIRS model). Choosing the model structure is dependent on the nature of the disease and population under consideration. For example, using an SIS (infected individuals return to the susceptible state) to model HIV, or an MSIR model (initial maternal-derived immunity) in the case of measles.\cite{vynnycky} 

It should be noted that there are further considerations to take when modelling real epidemics. For example, the inclusion of birth and death rate, seasonal dynamics, stochasticity and age-dependent interactions.\cite{vynnycky} However, the basic principles discussed above are sufficient to begin considering how we might model the spread of other epidemic processes. 

With the solid theoretical basis described above, advanced mathematical and computational models are becoming increasingly central to making public health decisions. One recent application of mathematical models in epidemiology was to describe and predict the dynamics of an epidemic in real time.\cite{kerkhove} A study by Tizzoni et al. used a Monte Carlo Maximum Likelihood (MCML)-based approach on historical data from the 2009 flu pandemic to develop a global stochastic simulation model, referred to as GLEAM, to obtain basic model parameters.\cite{gleam, gleam2} (Note that this project will aim to similar methodologies to fit epidemic models in real time, and a brief overview of maximum-likelihood estimation and least squares estimation is provided in Box 1). Tizzoni et al. used GLEAM to estimate the seasonal transmission ability of the 2009 H1N1 pandemic, generating forecasts for the activity peaks in the northern hemisphere. The robustness of this stochastic forecast was also explored as a function of data completeness by fitting the model using only partial data.\cite{tizzoni} 

Tizzoni et al. showed that the GLEAM model was in good agreement with
the actual 2009 epidemic data, even when only partial data was used
(for example, pre-exposure immunity and adherence to vaccination
campaigns. However, a key feature of the model is that it accounts for
the way in which  populations interact and connect, and it was shown
that model accuracy was reduced considerably when using only a partial
dataset for population mobility. The GLEAM model uses three layers: a
population layer (a grid representing the population of the world); a
mobility layer (using real flight data to represent travel between
cells in the grid); and an epidemic model (consisting of susceptible,
latent, symptomatic infectious able to travel, symptomatic infectious
unable to trabel, asymptomatic infectious and permanently covered
compartments). Indeed, consideration of multiple networks layers in
epidemic modelling is a growing area of consideration for real
infectious diseases, as it alows for the consideration of more
realistic population dynamics.\cite{gefm} \\\\

\newpage
\begin{framed}
{\begin{center}{\bf Box 1}\end{center}}
To fit model parameters in real time, we consider two methods to fit a continuous-time model to a given set of data: least squares and Maximum Likelihood Estimation (MLE).

{\bf Least squares} is the simpler of the two approaches, and assumes that the only source of variability in the data is measurement error (which is distributed symmetrically with a constant variance eg. Gaussian). By estimating the "least squares" for a set of parameters, we aim to find the set of parameters that minimises the sum of the squares of the errors (ie. the difference between an observed value and the fitted value). 

More formally, given a simple data set of $n$ points of the form $(x_i, y_i)$, we aim to minimise the following formula:
\begin{equation*}
S = \sum\limits_{i=1}^n r_{i}^2
\end{equation*}

Where $r_i$ is the residual for each point, given by the difference between the actual value of the dependent variable and the variable fitted by the model: $r_i = y_i - f(x_i, \beta)$

In the context of our SIR model, we aim to find the $\beta$ and $\gamma$ values that provide the best fitting SIR model for a given epidemic dataset (we will use the Nelder-Mead algorithm with \emph{R}).\cite{marily2013}

{\begin{center} \includegraphics[width=100mm]{sse.png}\end{center}}

Shown above is an example sum of squared errors (SSE) plot demonstrating how a particular value for $\beta$ minimises the SSE to a given dataset at around 0.003. This plot was created using a test flu dataset. Note that this plot assumes a $\gamma$ value of 1, though multiple parameters may be optimised simultaneously using R's \emph{optim} (Nelder-Mead) function.

{\bf Maximum-likelihood estimation} (MLE) is the method of estimating the parameters of a statistical model based on a given hypothesis and a set of data that has occured. This method essentially selects the set of values for the model parameters that maximise the likelihood function:
\begin{equation*}
	\mathcal{L} (\theta | x) = P(x | \theta)
\end{equation*}
In other words, we aim to find the set of parameters that  provide a model fit would be most likely to produce our given data. As above, we aim to find the set of parameters, $\beta$ and $\gamma$ that maximise this likelihood function. Specifically, we calculate the negative log-likelihood of the data given some combination of parameters. Methods using MLE are very common when fitting epidemic models in real time.\cite{white, hall, nishiura}
{\begin{center} \includegraphics[width=140mm]{logmle.png}\end{center}}
The graph above depicts the two-dimensional parameter space (the likelihood surface) for beta and gamma when fit to a set of epidemic data. Note that we use log values to avoid underflow/overflow.  Each point represents a separate fit to the data, and the height of the surface shows the negative log-likelihood of that parameter combination. In this particular example, we find that a log(beta) value of ~-6.11 and a log(gamma) value of ~-0.74 provide the best fit. We can also show the confidence intervals for each parameter in the form of a likelihood profile, as shown below.

{\begin{center} \includegraphics[width=140mm]{mle.png}\end{center}}

\end{framed}

\section*{Epidemic Phenomena on the Internet}
A more recent application of epidemic modelling methods is to the spread of information and trends. In particular, the continual development of the internet has opened up a vast area of research into the dynamics of social networks, viral marketing and computer security. It is quite easy to see the analogy between the spread of an online trend to the spread of an infectious disease: individuals have either been exposed to the trend or not; may be actively spreading the trend; or may have lost interest in the trend. The relatively recent surge of interest in online social networks and rapidly rising occurence of viral phenomena has brought with it an interest in understanding and modelling these trends. In this section, we will briefly discuss the history of epidemiology-based analyses of information dissemination and the resulting application of classical epidemiology to online phenomena.

The first application of epidemiology in a social context was made by Goffman and Newill in 1964 who directed attention to the analogy between the spreading of an infectious disease and the dissemination of information.\cite{goffman} This was closely followed by Daley and Kendal, who examined the spreading of rumours using mathematical epidemiology.\cite{goffman} As in Kermack and McKendrick's SIR model, Daley and Kendal used three compartments to describe their population: those individuals that had not heard the rumour, those that were actively spreading the rumour and those that were no longer spreading the rumour. The way in which these compartments interacted was described by parameters indicating those that heard the rumour and those that lost interest or `forgot' the rumour, corresponding to $\beta$ and $\gamma$. Daley and Kendal found that the fit was somewhat limited by the difference in behaviours of a rumour and an infectious disease. Specifically, the way in which individuals "lost interest"  in the rumour was not comparable to recovery from an infectious disease. Although not a perfect fit, Daley and Kendal's study did highlight the potential application of mathematical epidemiology in a social context.

More recently, the ever increasing relevance of the internet to modern day life has resulted in more studies being undertaken to model and predict the spread of trends and information on the internet. Bauckhage et al. present one such study, investigating the application of statistical models in describing the spread of internet memes.\cite{meme} That is, viral catch phrases, images or videos that spread through instant messaging, blogs, forums and social networking sites. Bauckhage et al. use \emph{Google Trends} data as an indicator of search frequency and therefore interest in the internet population. Classiying these as `fads', Bauckhage et al. go on to fit established statistical distributions to over 200 meme related time series compared to a fitted Log-Normal model. Using a multinomial maximum likelihood fit, the authors find that the Weibull, Gompertz and Frechet distributions all provided a better model of general trends for meme related search activity, suggesting that growth dynamics cannot be attributed to chance. The authors conclude that these dynamics can be described as a `hype cycle', encompassing a period of rapid uptake followed by a gradual loss of appeal. Although Bauckhage et al. did not use epidemic modelling, they demonstrated that mathematical modelling could be effectively used to describe online trend dynamics.

\begin{figure}[ht!]
\centering
\includegraphics[width=160mm]{statsdist.png}
\caption{Examples of the statistical distributions considered in Bauckhage et al.\cite{meme}}
\label{sir}
\end{figure}

The study by Bauckhage et al. demonstrates the applicability of mathematical modelling to the dynamics of internet memes, however research has also been undertaken to describe the spread of internet `celebrities'. Tweedle and Smith undertook one such study, investigating the usefulness of SIR modelling in describing the spread of popularity of the music artist, Justin Bieber.\cite{bieber} Tweedle and Smith demonstrated that an SIR model could be fit to \emph{Google Trends} search data relatively well. Furthermore, the study investigated how `media effects' might impact epidemic spread (for example, an album release or television appearance). It was found that the inclusion of media effects improved the usefulness of the model in describing the search trend data. Although a fairly `tongue in cheek' study, Tweedle and Smith demonstrated that SIR modelling could be successfully used to describe the spread of online popularity, particularly when additional media effects are considered.

A recent study by Nika et al. showed the potential for epidemiology to explain and predict outbreaks of internet-based information spreading, with the novelty that the size of the initial susceptible population was assumed to be unknown.\cite{marily2013} The aim of this study was to fit SIR and SEIR models to celebrity outbreaks on the internet in real time, improving model fit as the epidemic progresses. Using the Nelder-Mead algorithm with a least-squares-based objective function to fit SIR and SEIR models in real time (see Box 1), Nika et al. were able to demonstrate real time model fitting with unknown initial parameters. The authors validate their approach on synthetic epidemic data, a historical influenza epidemic and BitTorrent and YouTube video views. Both the SIR and SEIR models fit the synthetic and historical data well, and showed good predictive power. However, whilse the models were fit to the internet-based data with some success, the authors acknowledge limitations in their methodology, such as the generation of confidence intervals without due regard for parameter uncertainty. This study demonstrates a promising framework for fitting parameters to data in real time, though highlights the limitations of classic SIR modelling in its basic form.

One recent study using similar methodology that recently received public interest was by Cannarella and Spechler, who investigated the use of SIR modelling in describing public interest in OSNs, namely MySpace and Facebook.\cite{cannarella} Cannarella and Spechler used an adapted SIR model that modified the dynamics of the recovering population such that contact between recovered and infected individuals was required for recovery.  That is, individuals would only stop using the OSN if they came into contact with someone who had already stopped using it. The authors named this adaptation the `irSIR' model. As in the above studies, Cannarella and Spechler used \emph{Google Trends} search data as a proxy for service usage. Similar to Nika et al., the authors used the Nelder-Mean algorithm to find a best fit curve based on sum of squared error, also assuming that all initial parameters, including population size are unknown. The study found that in the case of MySpace, whilst the basic SIR model did not fit particularly well, the modified irSIR model provided a good fit to the adoption and abandonment phases of the OSN. When applied to the ongoing OSN, Facebook, the authors found that Facebook had reached peak popularity in 2012 and was in the early stages of abandonment, predicting that the OSN would reach 20\% of its maximum size by the end of 2014. The authors do concede that there exists an infinite range of possible slower declining solutions. 

Facebook posted a rebuttal to the study, using similar methodology to show that Princeton would cease to exists by 2021 based on Google search results.\cite{facebook} Although not a formal, peer revewed study, Facebook's rebuttal did highlight some shortcomings with the methodology employed by Cannarella and Spechler. Namely the assumption that Google searches were an indicator of usage, and also the flawed assumption that Facebook would not `evolve' to keep users. These studies highlight the importance of making valid assumptions and using appropriate data when attempting to study a rapidly evolving area such as OSNs.

With huge implications for marketing and commercial success, Interest in viral phenomena has become an area of particular interest outside of the academic community. As a result, commercial circles have also taken an interest in attempting to understand the spread of online trends. A review posted by the global strategic insight agency, \emph{Facegroup}, inadvertantly touched upon the application of SIR modelling in explaining the spread of viral videos.\cite{facegroup} The main conclusion that  \emph{Facegroup} came to was that there is no single model of virality. Rather, different types of viral videos could be spread in different ways, proposing `spike' and a `growth' types depending on their spreading pattern. In the `spike' case, videos tend to peak early and drastically drop in views within a week. In the latter case, videos achieve their peak views after a few days and decline slowly, interrupted by secondary peaks of interest. 
\newpage
\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{hatfield.png}
\caption{Graph \emph{Facegroup} depicting the uptake of the "Commander Hadfield" YouTube video, demonstrating the "growth" model as proposed by \emph{Facegroup}.\cite{facegroup}}
\label{sir}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{facegroup.png}
\caption{Graph from \emph{Facegroup} depicting the uptake of the "Dove Real Beauty Sketches" YouTube video, demonstrating the "spike" model as proposed by \emph{Facegroup}.\cite{facegroup}}
\label{sir}
\end{figure}
\newpage
Returning to an academic setting, Nika et al. recently undertook
another study to improve the fit of their real time fitting framework,
stating that, based on their previous study, a single epidemic is
inadequate to characterise a complex internet-based phenomena. This
may be because internet based trends may be influenced by multiple
underlying spreading mechanisms at different times, similar to the
`media effects' as described by Tweedle and Smith in their analysis of
`Bieber Fever'. Nika et al. took inspiration from Fourier analysis,
proposing that modelling and predicting internet-based phenomena could
be better described by considering multiple compartmental
epidemiological models. That is, an epidemic signal can be broken down
into a number of sub-epidemic models which, when recombined, can be
used as a predictive model (see Figure 1). Nika et al. go on to coin
the term \emph{synthedemic} from the field of syndemics - the idea
that infections can co-occur and interact with each other as well as
environmental factors. 

The term synthedemic is used to describe the co-occurence of a set of
infections, whether they are dependent or not. The aim of the study
was to account for the potential influence of multiple underlying
spreading mechanisms which may begin at different times by breaking an
incoming epidemic signal into component parts, and selecting the model
that best explains each component. Using only a classical SIR model
and exponential decay model as candidates, Nika et al. are able to
adequately characterise the evolution of synthetic data and four
real-world data sets from internet trends (BitTorrent downloads and
daily Youtube views) well. Please refer to Box 2 for a detailed
explanation of the theory used by Nika et al. Using synthetic double
epidemic data, the model by Nika et al. successfully predicts two
overlapping epidemics, predicting the peak of the second, spike
epidemic with a high RSquare value. The model is also fit successfully
to the BitTorrent downloads of two popular songs and a viral YouTube
video; detecting the presence of multiple outbreaks and exponential
decay of interest.
\newpage
\begin{framed}
{\begin{center}{\bf Box 2}\end{center}}
{\bf Methodology}:\\
The modelling procedure begins by considering a small, truncated
dataset of an epidemic outbreak. At each time point, an additional
data point is added until the end of the considered time frame is
reached. This might be a synthetic epidemic dataset, a historical
infectious disease epidemic, or some measure of interest in an online
phenomena (Nika et al. use BitTorrent downloads and daily YouTube
views). Nika et al. propose two candidate models that provide
theoretical analogues to the \emph{growth} and \emph{spike} trends as
proposed by \emph{Facegroup}: an SIR model to represent gradual
growth, and an exponential model to represent a rapid outbreak and
decay of public interest.\cite{facegroup}\\\\

At every time point, the multiple epidemic is optimised by attempting
to minimise the sum of squared error of the model against the data. At
each stage of fitting, the latest residuals are checked at each
additional time point for the presence of an additional epidemic
outbreak. If detected, a new epidemic is temporarily added to the
model from the list of candidate models. If the addition of this model
improves the $R^2$ value, then this additional epidemic is included in
all future model fittings. After the multiple epidemic model has been
optimised, an autoregressive model is fitted to capture the remaining
variability in the data. Finally, thefit of the multiple epidemic AR
model is assessed against benchmarked fitting procedures, such as a
single epidemic model, using a range of statistical tests, including
rSquare comparison.

{\bf Candidate Models}:\\
Let \emph{M} be the class of sub epidemic models under consideration: 

 \begin{equation*} M  = \{f_{1}^{(i)}, f_{2}^{(i)}\} \end{equation*}

Where $f_1^{(i)}$ and $f_2^{(i)}$ are defined as follows:
\begin{enumerate}[label=\Alph{*}.]
	\item $f_1^{(i)}$ denotes the SIR model $f_1^{(i)}(\theta^{(i)}, t)$ with parameter vector $\theta^{(i)} = [I_0^{(i)}, S_0^{(i)}, \beta^{(i)}, \gamma^{(i)}]$
\begin{equation*}
	\begin{split}
	&\frac{dS}{dt} = -\beta IS, \\
	&\frac{dI}{dt} = \beta IS - \gamma I, \\
	&\frac{dR}{dt} = \gamma I
	\end{split}
\end{equation*}
	\item $f_2^{(i)}$ denotes the Exponential decay model $f_2^{(i)}(\theta^{(i)}, t)$ with parameter vector $\theta^{(i)} = [I_0^{(i)}, \gamma^{(i)}]$
\begin{equation*}
	\frac{dI}{dt} = - \gamma I
\end{equation*}
\end{enumerate}
	
Parameter \emph{t} denotes a particular time, where $S_0^{(i)}$ and
$I_0^{(i)}$ denote the initial number of susceptible and infectious
individuals at time \emph{t}, whilst $\beta^{(i)}$ and
$\gamma^{(i)}$denote the infection and recovery rate. Furthermore, the
current number of sub epidemics within the overall model is given by
\emph{k}, where the combined value of the epidemic model at time
\emph{t} is given by the following formula:

\begin{equation*}
	\hat{y}(\theta ,t) = \sum\limits_{i=1}^k f^{(i)}(\theta^{(i)} ,t)
\end{equation*}
\end{framed}

Global mathematical models of epidemic processes provide a useful
insight into infectious disease dynamics; however, they make a number
of potentially unrealistic assumptions. Most notably, SIR models
assume complete mixing of the population. That is, each individual in
the population has an equal chance of coming into contact with every
other individual in the population. Although this assumption may hold
in certain scenarios (it is generally safe to assume that the internet
population observes random mixing), it may not be applicable to many
systems that are affected by local dynamics. Consider the population
of a town comprising of schools, offices and homes. Obviously a
student will mix much more frequently with other students and their
family rather than office workers. In such cases, the approach of SIR
based modelling may not be sufficient to capture the underlying
dynamics of the population. Many researchers have therefore developed
extended models to investigate stochasticity, multiple compartments
representing different subpopulations, branching processes and
chain-binomial models.\cite{computational}

One recent approach of particular relevance to the study of epidemic
phenomena in a social context is that of networked epidemiology: an
approach that encompasses individual behaviour, heterogenous
multiscale networks and the dynamical processes on these
networks. Although this particular project focuses on the application
of classical SIR modelling, it is worth discussing networked
epidemiology in brief to give a complete picture of the field.

\section*{Networked Epidemiology and Social Networks} 

We have briefly touched on the idea of networks in epidemiology with
reference to multi-layered epidemic models, however it is worth
discussing the basics behind network considerations.\cite{tizzoni,
  gefm} Networked epidemiology takes its inspiration from graph
theory, using nodes to denote individuals and edges to denote
interactions. 

Let \emph{G(V,E)} denote a contact graph on a population of \emph{V}
individuals, where each edge, \emph{e = (u, v) $\in$ E} denotes those
individuals, \emph{u, v $\in$ V} that come into contact. As in the SIR
model, each node might be in the \emph{ S, I} or \emph{R}
state. However, the key difference is that the infection may only
spread from \emph{u} to \emph{v} along an edge with a probability of
$\beta$(\emph{e, t}) at time instant \emph{t} after \emph{u} has
become infected. Similarly, a node only remains infected for a set
amount of time denoted by $\tau$(\emph{u}). After this time, the node
\emph{u} switches to state $R$. By considering this network model over
a given number of time steps, the dynamics of an epidemic taking place
in a network can be modeled. With this basic idea, it is easy to see
how real world networks structures and data can be used to vastly
improve models of epidemic processes in reality, thereby improving
approaches to vaccination and disease control programmes.\cite{danon}
On the other hand, the reality of transmission networks is not quite
so ideal, with information regarding population connectivity and
interactions being limited. In this section, we will discuss a sample
of studies that build on the concept of local network approaches, and
discuss some approaches that have been taken towards applying these to
online trends.

With the basis for another approach to epidemic modelling, we can
begin to consider general frameworks to describe local dynamical
processes. One such approach is called the graphical discrete
dynamical system (GDDS)\cite{bisset}, which is defined as a tuple
(\emph{G, F, $\pi$)}), where: \emph{G = (V, E)} represents the
underlying contact network;  \emph{F = \{f\textsubscript{v}$|$v $\in$
  V\}} is a set of local functions for each node, \emph{v}, to compute
the state of \emph{v} based on its neighbours; and $\pi$ is a schedule
that specifies the order in which the stages of the nodes are
updated. It is possible to view the configuration space of a GDDS as a
Markov chain, \emph{M}, where each node in \emph{M} is the state
vector of the node states in the GDDS, \emph{G}. For example, in the
SIR model these three states correspond to the susceptible, infectious
and recovered states. 

A conceptually simpler approach to local network modelling is that of
a cellular automata model. Cellular automata models take into
consideration local behaviour and heterogeneity by ascribing each
individual to a particular cell as part of a grid. Individual cells
may have a certain state, such as infected or susceptible, and may
interact with other cells depending on the assumptions made. Turner et
al. provide one such example of this type of model, proposing two
spatial host-pathogen models that are described as equivalent to a
density and frequency dependent global model.\cite{turner} Another
example is provided by Zanette and Risau-Gusman, who investigate the
effect of evolving connections between individuals affects the spread
of an infection. Zanette and Risau-Gusman use an SIS based model where
susceptible agents are able to break their links with infected agents
either temporarily or permanentely, showing that a moderate contact
reconnection frequency is sufficient to suppress
infection.\cite{zanette}  It is possible to stretch the analogy of
human networks to the internet, with towns and cities representing
various internet communities. However, the drastically higher level of
connectivity and transmission speed on the internet does limit the
applicability of these locally driven models.

Global models may provide a more appropriate approach to global
internet trends compared to locally driven models; however, research
into using network based models to describe the dynamics of social
networks has recently shown some success. The aim of such models are
largely to predict future viral trends. One such example is a study by
Altshuler et al., who investigated the use of social diffusion models
in trend prediction in an online social trading
community.\cite{altshuler} Altshuler et al. set out to answer the
following question: given a snapshot of a social network with some
behaviour occurences, what is the probability that these occurences
will result in a viral diffusion and a wide-spread trend? The authors
model the diffusion process using scale-free networks (ie. probability
that \emph{v} has \emph{d} neighbours follows a power law); taking
into account local fluctuations and heterogeneity. From this,
Altshuler et al. develop a theoretical mathematical model to
understand trend diffusion in social networks. However, as the authors
points out, their framework needs to be tested in the field by
conducting an active experiment in which the emergence of a trend is
predicted in real time.

\section*{Conclusion}
Research into the mathematical modelling of epidemic processes is now
an established field, with the majority of work based on the original
SIR model as proposed by Kermack and McKendrick.\cite{kermack}
Extensions of the SIR model take into account additional compartments
and inter-compartment dynamics, such as the inclusion of an `exposed'
group or of births and deaths. By customising model parameters and
structure, public health authorities and researchers can improve their
understanding of the way in which infectious diseases spread. For
example, using an SIS (infected individuals return to the susceptible
state) to model HIV, or an MSIR model (initial maternal-derived
immunity) in the case of measles.\cite{vynnycky} Understanding these
population dynamics in combination with the critical vaccination
threshold, given by the reproductive ratio of the virus, allows for
effective vaccination and control strategies. Particular topics of
note in recent years are the use of multi-layered epididemic models
and the use of maximum-likelihood estimations to predict the spread of
an epidemic in real time. \cite{tizzoni, gefm, white, hall, nishiura}

Although infectious diseases are the focus of mathematical modelling
of epidemic processes, a novel application is in the modelling of
internet-based phenomena and trends. As online social networks and
content sharing site become increasingly popular, the relevance of
understanding the dissemination of information online becomes an
increasingly important area of research both from an academic and
commercial perspective.\cite{cannarella, facebook} Studies in this
area are still at an early stage, though a few recent studies have
shown promising early results. \cite{marily2013, marily2014, bieber}
The study by Nika et al. shows promising results be considering the
presence of multiple, overlapping epidemics to describe one epidemic
phenomena in real time using a least-squares based fit. That is, the
popularity of a single online trend may be described through
considering its underlying sub-epidemic components. Nika et
al. consider two types of sub epidemic model as proposed by the
strategic insight agency, \emph{Facegroup}, and is the first study to
consider multiple overlapping epidemics to explain the spread of
internet trends. 

Although applying epidemic modelling techniques to the spread of
internet trends has shown promising results, there are a number of
limitations and flawed assumptions that must be considered. Firstly,
the analogy between an internet trend and an infectious disease is
limited. Nika et al. point out that whereas normal SIR modelling will
be able to make realistic assumptions or measurements regarding the
size of the initial susceptible population, this is not possible when
considering online trends. This $S_0$ value must therefore be treated
as an additional unknown parameter.\cite{marily2013} Furthermore, the
spreading mechanisms and lifecycle of internet trends are different to
those of infectious diseases. Whereas infectious diseases must be
passed on through physical contact, internet trends can be spread
instantaneously to any other user through a `tweet' or `share'. Trends
such as online videos or memes may also typically experience media
spikes when a video or celebrity is shown on television or highly
frequented web pages. The dangers of making such assumptions can be
observed in the study by Cannarella et al. who attempted to show that
Facebook would be abandoned by 2015, neglecting to consider that
Facebook will continue to `evolve' to keep users.\cite{cannarella,
  facebook} Proxies for interest in internet trends such as
\emph{Google Trends} search results should therefore be used with
caution.

Despite these limitations, promising early results encourage the
pursuit of further research. There are a number of other techniques
and models currently being investigated in infectious disease
modelling that might be applicable to the spread of internet
trends. For example, the use of maximum-likelihood estimations rather
than least squares;\cite{tizzoni} the use of multi-layered epidemic
models that might account for underlying social network structure (for
example, the spread of a viral video on or between social network
sites);\cite{altshuler, tizzoni} and the consideration of alternative
compartmental models or statistical distributions.\cite{meme}

With so many potential routes to follow, this project will focus on furthering the work done by Nika et al. to fit a multiple epidemic model in real time to the spread of online phenomena.

\subsection{Development Environment}
\subsubsection{Programming Languages}
There were a number of candidate programming languages, each with
their own strengths and weaknesses. In the end, R and C++ were chosen
for intitial and final implementations. Previous
approaches to epidemic and synthedemic model fitting frameworks use R
due to its readily available ODE solvers, optimisation functions and
graph plotting functionality.\cite{marily2013,marily2014} R therefore
provided an ideal means to implement the single and multiple epidemic
fitting frameworks initially. Once this initial model fitting
framework was implemented, we went on to provide a C++ implementation
with the aim of providing a faster, more transparent `from scratch'
fitting methodology.

At the start of the project, it was desirable to begin exploring and
understanding the theory behind epidemic modelling and otpimised model
fitting. As such, the first development consideration was to decide on
a language that was well adapted for easy implementations with a large
number of available packages and functions. R and Matlab were
candidates for this initial approach. Whilst Matlab has an arguably
better programming environment with better documentation, R has
already been shown to be effective in epidemic model fitting. The R
community provides a number of statistical analysis tools and is suited to dealing with non-typed data
sets, making it an ideal choice. These packages can easily be obtained
via the Comprehensive R Archive Network (CRAN).

The nature of parameter optimisation means that fitting a large number
of parameters simultaneously can be extremely slow, and an approach to
providing a faster implementation was to reimplement the model fitting
procedure in C++. C++ has been shown to be considerably faster than
both R and Matlab when solving stochastic neoclassical growth models,
suggesting that an efficient C++ implementation might provide a much
faster fitting framework than an R counterpart.\cite{languagespeed}
However, the trade off with run time speed is the fact that coding the
same algorithms and functions in C++ is very time consuming. Whilst
there are R packages readily available that allow parameter fit optimisation,
maximum likelihood estimations and graph plotting in only a few lines
of code, the equivalent functionality in C++ had to be implemented
from scratch. A significant challenge of this project was therefore to
find, adapt or create source code for the essential functions of the
model fitting framework. For graph plotting, a Gnuplot iostream was called from C++
code. 

Python and Java were also considered as potential languages for a
faster implementation. However, the relatively lower speed of Python and
unfamiliarity with Java meant that C++ remained the ideal choice.

It should be noted that whilst C++ provides an ideal way of speeding
up computational bottlenecks in the model fitting procedure (namely
the optimisation step), it it may still be desirable to call R
functions from within the C++ program. For example, the generation of
a likelihood profile. This can be achieved using the
Rcpp library if needed. Furthermore, the quick generation of synthetic
data with which to evaluate and develop the fitting framework is
clearly not a limiting factor. R therefore remained the ideal language
for syynthetic data generation using \emph{GillespieSSA} package,
exporting the data as a .csv file to be imported in the C++ implementation. 

 
\section{Single Epidemic Fitting}
In this section we explore the theory and implementation behind a
model fitting framework for single SIR models with unknown
parameters. To provide a simulation of real time model fitting, we
iteratively fit a new optimised model at each data point. Firstly, a
least-squares fitting procedure is implemented in R for a single SIR
epidemic where beta, gamma and S0 are assumed to be entirely
unknown. We then extend the implementation to include the start time
of the epidemic, t0 as a fourth unknown parameter. This also raises
the issue of epidemic outbreak detection and model selection, which we
will revisit in section SECTIONNNN!. We go on to use a maximum
likelihood based approach which allows for the generation of
confidence intervals. Finally, we reimplement the above approaches in
C++ to provide a much faster fitting framework.

\subsection{Data Management}
The data that we wish to characterise is the change in number of
infected individuals over time, whether infected individuals is
defined as individuals infected with a disease or individuals that
have viewed a particular \emph{YouTube} video or `liked' a particular
\emph{Facebook} post. An example of this type of data is shown in
Figure XX. It is very easy to deal with data in \emph{R}, as the use
of `data frames' allows for .csv files of epidemic data to be easily
imported and manipulated. In C++, we also import data as .csv files,
but as data must be stored in vectors, initial data formatting is more
crucial. 


\begin{center}
\begin{figure}[ht!]

\includegraphics[width=15cm]{simplesir.png}
\caption{Example synthetic epidemic data generated using the
  \emph{GillespieSSA} algorithm in \emph{R}. In this model: $beta =
  0.0015, gamma = 0.1, S0 = 800, I0 = 1$.}
\end{figure}  
\end{center}

We use the results of solving ODEs with known parameters and
independent runs of the \emph{GillespieSSA} algorithm to test the
framework's ability to find the true model
parameters. \emph{GillespieSSA} provides an easy to use, extensible
means of generating simulated trajectories of finite population
continuous-time models. Figure XX shows a basic implementation of the
SIR model in R using \emph{GillespieSSA}.

\begin{center}
\begin{figure}[!]

\begin{lstlisting}[language=R, style=customc]
function(params, I0, time, i){
    # Define parameters
    parms <- c(beta=params[1],gamma=params[2])
    
    # Define system
    x0 <- c(S=params[3], I=I0, R=0)  # Initial state vector
    nu <- matrix(c(-1,0,1,-1,0,1),nrow=3,byrow=T) # State-change matrix
    a  <- c("beta*S*I", "gamma*I")         # Propensity vector
    tf <- time                             # Final time
    simName <- "Kermack-McKendrick SIR"

    # Run the simulations
    nf <- layout(matrix(c(1,2,3,4),ncol=2, byrow=T))
    
    # Direct method
    set.seed(i)
    out <- ssa(x0,a,nu,parms,tf,method="ETL",tau=1, simName,verbose=FALSE)
    return(out.data)
}
\end{lstlisting}
\end{figure}
\end{center}



\subsection{Candidate Models}
As we aim to find a model that best describes our data, the next
consideration is the creation of data that best fits our epidemic data
through candidate model equations. With a candidate model in mind,
namely the \emph{SIR} model, and a set of candidate parameters (beta,
gamma and S0), we find the data described by the model and parameters
and compare this to the epidemic data. When implementing the model fitting
framework in \emph{R}, we utilise the \emph{ode} function from the
\emph{deSolve} package to return a matrix of values calculated from a
given set of parameters, a set of ODEs and a desired time frame. A
simple \emph{R} implementation of the \emph{SIR} is shown in Figure~\ref{fig:sirR}.

\begin{center}
\begin{figure}[ht!]
\centering
\begin{lstlisting}[language=R, style=customc]
function(time, data, parameters) {
  S <- data[1]
  I <- data[2]
  R <- data[3]

  beta <- parameters[1]
  gamma <- parameters[2]

  dS <- -beta*S*I
  dI <- beta*S*I - gamma*I
  dR <- gamma*I

  list(c(dS,dI,dR))
}
\end{lstlisting}
\label{sirR}
\end{figure}
\end{center}

With a model solving framework and a set of data that we wish to fit,
one can begin to visualise how the model fitting process might take
place. Even with completely unknown parameters, candidate models can
be generating by choosing parameters that might fit the data. Figure~\ref{fig:sircurves} depicts how using various model parameters can give different
shaped curves. 

\begin{center}
\begin{figure}[ht!]

\includegraphics[width=15cm]{Rplot01.png}
\caption{Depiction of how various candidate model parameters give
  varying levels of fit for a given dataset}
\label{sircurves}
\end{figure}  
\end{center}

As a theoretical aside, it should be highlighted that the
\emph{ode} function uses the LSODA integration method by default,
based on FORTRAN code. The benefit of the LSODA method is that is
automatically switches between stiff and non-stiff systems, and is
very robust. However, in our C++ implementation, we provide a `from
scratch' ODE solver using the Runge-Kutta method in an attempt to
speed up the model fitting process. A theoretical introduction to ODE
solving is provided in Box 4.  

\newpage
\begin{framed}
{\begin{center}{\bf Box 3}\end{center}}
{\bf Solving Ordinary Differential Equations}:

The epidemic models take the form of ordinary differential
equations (ODEs), wherein the dynamics of the population are described by the
transition of individuals between different compartments. In the case
of the the \emph{SIR} model, individuals transition from susceptible,
to infected and finally to recovered. As discussed in Box 1, the rate
of transition between these states depends on the model parameters
(namely beta and gamma), as well as the number of individuals
currently in each compartment. 

For the purpose of model fitting, it is necessary to calculate the
number of individuals in each compartment at each time point. This
requires the set of ODEs to be solved. Given a set of parameters and
initial compartment sizes, we wish to find the number of individuals
over the course of the epidemic at each time point. For a simple
differential equation, it is possible to find the closed form
solutions. Given a function, \emph{g}, we wish to find the solution
such that:

\begin{equation}
\begin{split}
  &Y'(t) = g(t)\\
  &Y(t) = \int g(s)ds+c
\end{split}
\end{equation}

where \emph{c} is an arbitrary constant, and the value of Y(t) can be
obtained at a given time point: \begin{equation} Y(t_0) = Y_0 \end{equation}

In the case of
first-order differential equations (as is our case), we take the above
equation as the initial value condition and are presented
with an initial value problem of the form:\cite{atkinson} 

\begin{equation}
\begin{split}
  &y'(t) = f(t,y(t)),\\
  &y(t_0) = y0
 \end{split}
 \end{equation}

It is often impractical to derive analytical solutions to
differential equations. In the case of models in epidemiology, the
first-order different equations are often
non-integrable.\cite{shabbir} In the case of \emph{SIR} modelling and similar sub
types, considerable work has been undertaken to attempt to solve
\emph{SIR} models analytically using Lie analysis and homotopy
analysis.\cite{nucci, khan} Such approaches are difficult to implement
and are not fit for the purpose quickly solving ODEs in a
generalisable way. We therefore turn to numerical
analysis.

Numerical methods are used to find numerical approximations to the
solutions of ODEs rather than solving them analytically. For the
purpose of obtaining population values that can be used in model fit
assessment, such numeric approximations are sufficient. The simplest
numericla method for solving the initial value problem is \emph
{Euler's method}, which involves finding an approximate nearby point
on the curve by moving along a line tangent. Euler's method forms
the basis for a highly popular group of
methods for solving initial value problems known as Runge-Kutta
methods, which are relatively easy to implement. 

The most basic member of the Runge-Kutta methods is simply known as
the ``classical Runge-Kutta method'', and is the method that we chose
to implement in C++. Given the above initial value problem and an
initial condition, we can attempt to find later values of y(t) with
the following definitions:

\begin{equation}
\begin{split}
  &y_{n+1} = y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\\
  &t_{n+1} = t_n + h
 \end{split}
\end{equation}

for \emph{n} = 0, 1, 2, 3..., using

\begin{equation}
\begin{split}
  &k_1 = f(t_n,y_n),\\
  &k_2 = f(t_n + \frac{h}{2},y_n+ \frac{h}{2}k_1),\\
  &k_3 = f(t_n + \frac{h}{2},y_n+ \frac{h}{2}k_1),\\
  &k_4 = f(t_n+h,y_n+hk_3)
\end{split}
\end{equation}

Note that $y_{n+1}$ is the Runge-Kutta approximation of $y(t_{n+1})$,
where $y_{n+1}$ is determined by the weighted average of four
increments of $y_n$ at interval size, \emph{h}, with the estimated
slope specified by the right hand side of the differential
equations. Note that greater weighting is given to the increments at
the midpoint of the chosen interval.
\end{framed}

\subsection{Parameter Optimisation}
We now have no clearly identified our problem and the
means by which we can attempt to solve it. That is, given a set of
epidemic data, can we find a set of parameters that, when used to
solve a set of ODEs, generate a model that accurately fits the
data. Firstly, we select a set of initial test parameters. We then
attempt to optimise these parameters to best fit our data using an
optimisation algorithm alongside an objective function measuring model
fit.

\subsubsection{Initial Test Parameters}
In classical epidemiology, it is often possible to obtain estimates of
many important model parameters based on disease biology and
population dynamics. For example, the initial susceptible population
of an isolated influenza outbreak might be estimated as the school-aged
population of a country. Similarly, the infection and recovery rates of a
new strain of virus might be estimated based on phylogenetic relationships
to previous viruses of known parameters.\cite{volz} Another potential method is to measure
transmission rates in experimental populations, as demonstrated by
Bouma et al., who estimated the transmission parameters of the H5N1
avian influenza virus using a small number of birds in an experimental
tranmission study.\cite{bouma} However, it is easy to imagine
situations where parameter estimation might be infeasible. Estimating
the susceptible population size for a viral \emph{YouTube} video, for
example, might be difficult. Do we assume that the entire internet
population is at risk of exposure, or will the video be limited to only
certain internet communities? Such a scenario is not unimaginable for
infectious diseases. Should a new, uncharacterised disease arise in
only an unknown demographic, the task of estimating model parameters
becomes very difficult.

In scenarios where parameter estimation is infeasible, we aim to find
the true model parameters without making any assumptions as to where
they might lie other than within a realistic range. In the presented
model fitting framework, we begin the parameter optimisation procedure
with random parameter values taken from a realistic range with
reasonable limitations imposed. For example, we seed the optimisation
procedure with a random beta value between 0.0001 and 0.01. It is also
important to ensure that a number of realistic conditions are adhered
to:

\begin{enumerate}
  \item The basic reproductive ratio must be sufficient to allow an
    epidemic to take off, $R0 > 1$. For this to be adhered to, gamma
    must be greater than beta.
  \item The initial number of susceptible individuals, $S_0$ must be
    positive and within a reasonable range. Seeding with an
    unrealistically high value might prevent the optimisation
    procedure from converging on an optimal solution.
  \item The initial number of infected individuals, $I_0$, must be
    greater than 0. Whilst $I_0$ does not necessarily need to be bound
   from above by $S_0$, it is more realistic to assume that $S_0$ is
   much greater than $I_0$. 
\end{enumerate}

One heuristic for estimating the start value of $I_0$ is to take the
first data point as the initial number of infected
individuals. However, this causes the model to be highly dependent on
first data point, neglecting to consider that the first data point
might not represent the start of the epidemic. A more reasonable
approach would be to consider $I_0$ as another unknown parameter to be
optimised, or to assume that there is initially only one infected
patient, or `patient zero'.

\subsubsection{Parameter Transformations}

\subsubsection{The Objective Function}
With a set of model equations, potential model parameters and the
resulting model data at each time point, the next step is to assess
how well the proposed model fits the given data. It is only through
quantifying this measurement that we can then go on to find the best
fitting model parameters. As discussed in section BACKGROUND, the
first assessment of fit that we implement is the least squares
fit. This uses the total squared difference between each the model
value and dataset value at each time point. The smaller this `sum of
squared errors' (SSE), the better the model fit. Clearly our aim is to find
the set of parameters that minimises this SSE. A central part of the
model fitting framework is therefore the implementation of this
`objective function' (Figure XX). Please refer to Box 2
for a discussion of least squares fitting.

\begin{center}
\begin{figure}
\begin{lstlisting}[language=R, style=customc]
function(params, data) {
    t <- data[,1]
    cases <- data[,2]
    beta <- params[1]
    gamma <- params[2]
    S0 <- params0[3]
    I0 <- 1
    R0 <- 0
    out <- as.data.frame(ode(y=c(S=S0,I=I0,R=R0), times=t,closed.sir.model,parms=c(beta,gamma), atol=1e-15,hmax=1/120))
    sse<-sum((out\$I-cases)^2)
}



\end{lstlisting}
\caption{The above function takes a set of parameters and a set of
  epidemic data. The \emph{ode} function is then uses the lsoda solver
  evaluate the \emph{SIR} model using the provided parameters. The sum of squared
  errors is then calculated from the generated model and provided data.}
\end{figure}
\end{center}


\subsubsection{Optimisation Procedure}
The next step in the model fitting framework is an implementation of
an optimisation procedure to find the set of parameters that minimise
the result of the objective function. In the initial \emph{R}
implementation, this is done by passing the initial seed parameters,
the objective function and the data to the \emph{optim}
function. \emph{Optim} uses the Nelder-Mead algorithm to find the set
of parameters in the parameter space that return the minimum objective
function value. That is, the set of parameters that evaluate to a
model that most closely fits the provided data. Box 4 provides a
theoretical overview of the Nelder-Mead algorithm.

- Discuss other optimisation algorithms that optim uses
- Discuss the C++ implementation of Nelder-Mead, namely how we adapted
 a primitive implementation to allow for an object of any class and a pointer to
 any function can be passed along with a vector of any class
 parameters.
- Discuss the importance of iterations and error tolerance

Figure XX illustrates the results of running \emph{optim} on a
\emph{GillespieSSA} generated model with known parameters.

\begin{centering}
\begin{figure}
\includegraphics[width=15cm]{simplefit.png}
\caption{The Gillespie algorithm is run with original parameter values of $\beta =
  0.001, \gamma = 0.1, S_0 = 500$. The \emph{optim} function returns
  fitted parameter values of $\beta = 0.0008018066, \gamma = 0.1273486, S_0
  = 618$. This results in a SSE of 5120.06.}
\end{figure}
\end{centering}



\newpage
\begin{framed}
{\begin{center}{\bf Box 4}\end{center}}
{\bf The Nelder-Mead Algorithm}:\\
The Nelder-Mead algorithm, or simplex search algorithm, is one of the
best known and commonly used algorithms for multidimensional
unconstrained optimisation without derivatives. The algorithm is
relatively simple to understand and implement, which makes it an ideal
candidate for solving parameter estimation problems. The method
ultimately approximates a local optimum of a problem with $N$
variables when provided with an objective function to be minimised.

Given a nonlinear function, $f : {\mathbb
  R}^n \to {\mathbb R}\ .$, the Nelder-Mead algorithm uses a
simplex-based search method to minimise the $f$, where a simplex, $S$
in ${\mathbb R}^n$ is defined as the convex hull of $n + 1$ vertices,
$x_0,...,x_n \in {\mathbb R}^n$. In the case of ${\mathbb R}^2$, the
simplex is a triangle, whereas in the case of ${\mathbb R}^3$, the
simplex is a tetrahedron. In the case of epidemic model fitting, each
vertex of the simplex corresponds to a set of model parameters.

Starting with a set of $n+1$ points (the initial `seed' parameters)
and a corresponding set of function values at the vertices, $f_i :=
f(x_i),$ for $j = 0,...,n$, the Nelder-Mead method performs a sequence
of transformations on the working simplex S with the aim of decreasing
the function values at its vertices. Once the method has satisfied
some minimisation condition, whether it be a number of steps or a
desired minimisation range, the final simplex can be used to return
the optimised parameters.

To summarise, the Nelder-Mead algorithm follows the following set of
steps:

\begin{enumerate}
\item Construction of the initial working simplex, $S$, around an initial
  point based on initial parameters
\item Repeat the following steps until maximum number of iterations
  reached or minimisation condition satisfied:
  \begin{enumerate}
  \item Calculate the function value for the current working simplex
  \item Check if terminiation criteria met
  \item If not met, transform towards the best vertex of the working simplex to give new
    vertex values with the following sub steps:
    \begin{enumerate}
      \item Determine the order of vertices in terms of function
        values (from best to worst)
        \item Calculate the centroid, $c$ of the side opposite the
          worst vertex
\item Compute a new working simplex from the current simplex using a
  series of transformations
\end{enumerate}
  \end{enumerate}
\item Return the best vertex of the current simplex, S, along with
  its associated function value.
\end{enumerate}

In the transformation step, replacing the worst vertex is attempted
using reflection, expansion or contraction with respect to the best
side. Firstly, the worst vertex is replaced with a reflection of the
best vertex. If this point provides an improvement on the current best
value, then the simplex is expanded towards the new point. Otherwise,
the simplex is contracted towards the current best point.If
successful, then the new point replaces the worst vertex of the
working simplex. If not, then the simplex is shrunk towards the best
vertex. A later addition of the algorithm is to shrink the entire
simplex in the event of failed contractions, though this is a rare and
slow step.

\begin{center}
  \includegraphics[width=15cm]{nelder.png}
  \captionof{figure}{A) Reflection. B) Exansion. C) Outside
    contraction. D) Inside contraction. E) Shrink transformation}
\end{center}

Ultimately, the Nelder-Mead method is a fast and relatively simple
algorithm for obtaining a good reduction in function value in a
relatively small number of function evaluations. For optimisation
problems where a precise optimum (eg. parameters are subject to noise)
is not necessarily required, the Nelder-Mead method is ideal. However, it is possible for the
Nelder-Mead algorithm to undertake an extremely high number of
iterations with little to no function value improvement at a region
far from the actual minimum. A heuristic solution to this problem is
to restart the algorithm at multiple start points and to only allow a
small number of iterations during each run.\cite{nelder,singer}
 
\end{framed}

\subsection{Evaluating Goodness of Fit}
Although the SSE provides a value which the optimisation process can
aim to minimise, it's magnitude is largely meaningless without
appropriate context. We therefore use the coefficient of
determination, $R^2$, to provide an evaluation of model fit. $R^2$ is
a widely used measure of goodness of fit in statistical modelling, and
provides an ideal means for comparison between different model
fits. $R^2$ provides a measure of what proportion of total variation
in the data is explained by the model. A mathematical definition of
the $R^2$ measure is provided in Box 5.

\newpage
\begin{framed}
{\begin{center}{\bf Box 4}\end{center}}
{\bf The Coefficient of Determiniation}:\\

Consider a data set with observed values $y_i$. We would like to assess how
well a set of predicted values $f_i$ fits our data. Firstly, we
consider the amount of variability in our data set using the sum of
squares:

\begin{equation}
  SS_{tot} = \sum\limits_{i}(y_i - \bar{y})^2
\end{equation}

Where $\bar{y}$ denotes the mean of the observed:

\begin{equation}
  \bar{y} = \frac{1}{n}\sum\limits_{i=1}^n(y_i)
\end{equation}

Next, we calculate the sum of squares of the residuals. That is, the
amount of discrepancy between the data and the predicted values:

\begin{equation}
SS_{res} = \sum\limits_{i}(y_i - f_i)^2
\end{equation}

The final step is then to evaluate the amount of unexplained variance of
the model with the total variance of the data. This gives us the
\emph{coefficient of determination}, $R^2$:

\begin{equation}
  R^2 \equiv 1 - \frac{SS_{res}}{SS_{tot}}
\end{equation}

The resulting value is usually a number between 0 and 1, where 1
suggests that the predicted values explain all of the variance of the
data (a perfect fit), and a value close to 0 implies that it explains
very little of the data
variance (a poor fit). Values of less than 0 are also possible, which indicates
that the mean of the data provides a better model fit than the model.

\end{framed}

\subsection{}



\newpage
\section{Multiple Epidemic Fitting}


\newpage
\section{Evaluation}


\newpage
\section{Conclusions and Future Work}

\begin{thebibliography}{1}
	\bibitem{who}
		World Health Organisation,
		\emph{Major causes of death}.
		2014,
		Available at \url{http://www.who.int/mediacentre/factsheets/fs310/en/index2.html}
	\bibitem{dawkins}
		Dawkins, R (1976) The Selfish Gene. Oxford University Press, Oxford, UK.
	\bibitem{hartmann}
		W. Hartmann, P. Manchanda, H. Nair, M. Bothner, P. Dodds, D. Godes, K. Hosanagar, and C. Tucker. Modeling Social Interactions: Identification, Empirical Methods and Policy Implications. \emph{Marketing Letters}, 19(3):287-304, December 2008.
	\bibitem{bieber}
		V. Tweedle and R. J. Smith. A mathematical model of Bieber Fever: The most infectious disease of our time. Understanding the dynamics. March 2012
	\bibitem{marily2013}
		M. Nika, G. Ivanova, WJ. Knottenbelt (2013) On celebrity, epidemiology and the internet. In: Proc. 7Th International Conference on Performance Evaluation Methodologies and Tools (VALUE-TOOLS). Turin, Italy
	\bibitem{meme}
		C. Bauckhage, K. Kersting, and F. Hadiji. Mathematical Models of Fads Explain the Temporal Dynamics of Internet Memes. In Proc. ICWSM. AAAI, 2013.
	\bibitem{wang}
	H. Li, H. Wang, J. Liu, and K. Xu. Video sharing in online social networks: measurement and analysis. In \emph{Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video}, NOSSDAV '12, pages 83-88, New York, NY, USA, 2012. ACM
	\bibitem{hu}
		H.-W. Hu and S.-Y. Lee. Study on inuence diffusion in social network. \emph{International Journal of Computer Science and Electronics Engineering} (IJCSEE), 1, 2013.
	\bibitem{singer}
		M. Singer, \emph{Introduction to Syndemics}. Wiley.
	\bibitem{bastos}
		F. Bastos et al. Co-infection with malaria and HIV in injecting drug users in Brazil: A new challenge to public health? \emph{Addition} 94:1165-174, 1999
	\bibitem{marily2014}
	M. Nika, T. Wilding, WJ. Knottenbelt. Going Multi-Viral: Synthedemic Modelling and Prediction of Internet-based Phenomena. \emph{In press}
	\bibitem{computational}
	M. Marathe and A. K. S. Vullikanti. Computational
        epidemiology. \emph{Commun. ACM}, 56(7):88-96, July 2013.
        \bibitem{languagespeed} S. Boraan Aruoba,
          J. Fernandez-Villaverde. A Comparison of Programming
          Languages in Economics. \emph{NBER Working Paper} No. 20263,
          June 2014.
	\bibitem{variolation}
	G. Williams (2010). \emph{Angel of Death}. Basingstoke: Palgrave Macmillan.
	\bibitem{hippo}
	F. Adams, E. Kelly (2006) \emph{The Genuine Works of Hippocrates}. Kessinger Publishing. URL http://books.google.co.uk/books?id=X7uR4S5YvhQC
	\bibitem{brauer}
	 Brauer, F., van den Driessche, P. and Wu, J. editors.  Mathematical Epidemiology. \emph{Lecture Notes in Mathematics}. Springer Verlag, 1945 
	\bibitem{snow}
	J. Snow (1855) \emph{On the Mode of Communication of Cholera}. John Churchill. URL http://books.google.co.uk/books?id=-NO\_AAAAcAAJ.
	\bibitem{kermack}
	WO. Kermack, AG. McKendrick (1991) Contributions to the mathematical theory of epidemics-I. 1927. \emph{Bull Math Biol} 53: 33-55.
	\bibitem{vynnycky}
	E. Vynnycky, R. White (2010) An Introduction to Infectious Disease Modelling.
	\bibitem{anderson}
	RM. Anderson, RM. May (1991) Infectious Diseases of Humans: Dynamics and Control. \emph{Oxford University Press}
	\bibitem{diekmann}
	O. Diekmann, JAP. Heesterbeek, JAJ Metz. On the definition and the computation of the basic reproduction ratio $R0$ in models for infectious diseases in heterogenous populations. \emph{Journal of Mathematical Biology} 1990:28;365-382
	\bibitem{goffman}
	D. J. Daley and D. G. Kendall. Epidemics and rumours. \emph{Nature}, 204(4963):1118-1118, 12 1964
	\bibitem{cannarella}
	Cannarella, J and Spechler, JA (2014) Epidemiological Modeling of Online Social Network Dynamics. arXiv:1401.4208 [cs.SI]
	\bibitem{facebook}
Develing, M; Adamic, L; Taylor, S. Debunking Princeton. URL: https://www.facebook.com/notes/mike-develin/debunking-princeton/10151947421191849
	\bibitem{facegroup}
	Facegroup. How stuff spreads: How videos go viral part I. URL: http://www.facegroup.com/how-videos-go-viral.html	
	\bibitem{bisset}
	K. Bisset et al. Interaction-based HPC modeling of social, biological, and economic contagions over large networks. In \emph{Proc. of Winter Simulation Conference} (2011), 2933-2947.
	\bibitem{turner}
	J. Turner, M. Begon, RG. Bowers. Modelling pathogen transmission: the interrelationship between local and global approaches. \emph{Proc. R. Soc. Lond.} B 2003:270;105-112.
	\bibitem{zanette}
	D. Zanette and S. Risau-Gusman. Infection spreading in a population with evolving contacts. \emph{Journal of Biological Physics}, 34(1-2):135-148, 2008.
	\bibitem{altshuler}
	Y. Altshuler, W. Pan, AS. Pentland. Trends Prediction Using Social Diffusion Models. In \emph{Proceedings of the international conference on social computing, behavioral-cultural modeling, and prediction}. Lecture notes in computer, Springer, pp 97–104.
	\bibitem{cockman}
	P. Cockman et al., Improving MMR vaccination rates: herd immunity is a realistic goal. \emph{BMJ} 2011;343:d5703
\bibitem{sars}
	M. Lipstitch, T. Cohen, B. Cooper et al., Transmission dynamics and control of severe acute respiratory syndome. \emph{Science} 2003;300(5627):1966-1970
\bibitem{kerkhove}
	MD. Van Kerkhove et al., Studies needed to address the public health challenges of the 2009 H1N1 influenza pandemic: insights from modeling. \emph{PLoS Med} 2010;7:e1000275
\bibitem{tizzoni}
	M. Tizzoni et al., Real-time numerical forecast of global epidemic spreading: case study of 2009 A/H1N1 pdm. zemph{BMC Medicine} 2012;10:165
\bibitem{gleam}
	D. Balcan et al., A Seasonal transmission potential and activity peaks of the new influenza A/H1N1: a Monte Carlo likelihood analysis based on human mobility. \emph{BMC Med} 2009;7:45
\bibitem{gleam2}
	D. Balcan et al., Multiscale mobility networks and the large scale spreading of infectious diseases. \emph{Proc Natl Acad Sci USA} 2009;106:2184-21489
\bibitem{gefm}
	FD. Sahneh, C. Scoglio, P. Van Mieghem, Generalized Epidemic Mean-Field Model for Spreading Processes Over Multilayer Complex Networks. \emph{IEEE/ACM Transactions on Networking} 2013;21(5): 1609-1620
\bibitem{white}
LF. White, M. Pagano, A likelihood-based method for real-time estimation of the serial interval and reproductive number of an epidemic. \emph{Statist. Med.} 2008;27:2999-3016
\bibitem{hall}
IM. Hall, R. Gani, HE. Hughes, S. Leach, Real-time epidemic forecasting for pandemic influenza. \emph{Epidemiol. Infect.} 2007;135:372-385
\bibitem{nishiura}
H. Nishiura, Real-time forecasting of an epidemic using a discrete time stochastic model: a case study of pandemic influenza (H1N1-2009). \emph{BioMedical Engineering OnLine} 2011;10:15
\bibitem{gillespie}
	M. Pineda-Krch. GillespieSSA: Implementing the Gillespie Stochastic Simulation Algorithm in R. \emph{Journal of Statistical Software}, 2008;25(12):1-18
\bibitem{cdc}
	CDC FluView Web Portal. URL: http://gis.cdc.gov/grasp/fluview/fluportaldashboard.html
\bibitem{danon}
	L. Danon et al., Networks and the Epidemiology of Infectious
        Disease. arXiv:1011.5950 [physics.soc-ph]
\bibitem{shabbir}
        G. Shabbir, H. Khan, MA. Sadiq, A note on Exact Solution of SIR and
        SIS epidemic models. arXiv:1012.5035 [math.CA]
\bibitem{nucci}
        MC. Nucci and PGL. Leach, An integrable SIS model. \emph{J. Math. Anal. Appl.} 2004;290:506-518
\bibitem{khan}
        H. Khan, RN. Mohapatra, K. Vajravelu and SJ. Liao, The explicit
        series solution of SIR and SIS epidemic
        models. \emph{Appl. Math. Comput.} 2009;38:653-669
\bibitem{atkinson}
        K. Atkinson, W. Han, D. Steward \emph{Numerical Solution of
          Ordinary Different Equations}. Wiley, Hoboken (2009); Page 5
\bibitem{nelder}
  JA. Nelder and R. Mead. A simplex method for function
  minimization. \emph{The Computer Journal},
  1965;7(4):308-313. \url{http://comjnl.oxfordjournals.org/content/7/4/308.full.pdf}.
\bibitem{singer}
  S. Singer and J. Nelder. Nelder-Mead algorithm. \emph{Scholarpedia},
  2009;4(7):2928.
  \url{http://www.scholarpedia.org/article/Nelder-Mead_algorithm}. 
\bibitem{volz}
  EM. Volz, SL. Kosakovsky Pond, MJ. Ward, AJ. Leigh Brown and
  SDW. Frost, Phylodynamics of Infectious Disease
  Epidemics. \emph{Genetics} 2009;183(4):1421-1430
\bibitem{bouma}
  A. Bouma, et al., Estimation of Transmission Parameters of H5N1
  Avian Influenza Virus in Chickens. \emph{PLoS Pathog}. 2009;5(1):e1000281
\bibitem{aron}
  JL. Aron & IB. Schwartz, Seasonality and period-doubling
  bifurcations in an epidemic model. \emph{J. Theor. Biol}. 1984;110:665-679
\bibitem{burnham}
  KP. Burnham & DR. Anderson, Model Selection and Multimodel
  Inference: A Practical Information-Theoretic Approach (2nd ed.),
  2002, \emph{Springer-Verlag}


\end{thebibliography}

\section*{Appendix}



\end{document}
