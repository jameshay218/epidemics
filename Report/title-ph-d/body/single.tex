\chapter{Single Epidemic Fitting}
\label{ch:single}

In this section we explore the theory and implementation behind a
model fitting framework for single SIR models with unknown
parameters. To provide a simulation of real time model fitting, we
iteratively fit a new, independently optimised model at each data point. Firstly, a
least-squares fitting procedure is implemented in R for a single SIR
epidemic where beta and gamma are assumed to be entirely
unknown. We then extend the implementation to include the number of
initial susceptible individuals, S0; the start time
of the epidemic, t0; and the initial number of infected individuals, I0. This also raises
the issue of epidemic outbreak detection and candidate model selection, which we
will revisit in section SECTIONNNN!. We go on to use a maximum
likelihood based approach which allows for the generation of
confidence intervals. Finally, we reimplement the above approaches in
C++ to provide a much faster fitting framework.

\section{Epidemic Data}
The data that we wish to characterise is the change in number of
infected individuals over time. In this context, the term `infected individuals' may be
defined as individuals infected with a disease, or individuals that
have viewed a particular \emph{YouTube} video or `liked' a particular
\emph{Facebook} post. Examples of this type of data are shown in
Figure~\ref{figure:examples}. \emph{R} is ideally suited for the easy management and
manipulation of data through the use of the `data frame' type. In C++,
we import data as .csv files and carry out all manipulation and use
using vectors.


\begin{center}
\begin{figure}[ht!]

\includegraphics[width=15cm]{simplesir.png}
\caption{Example synthetic epidemic data generated using the
  \emph{GillespieSSA} algorithm in \emph{R}. In this model: $beta =
  0.0015, gamma = 0.1, S0 = 800, I0 = 1$.}
\label{figure:examples}
\end{figure}  
\end{center}

We use the results of solving ODEs with known parameters and
independent runs of the \emph{GillespieSSA} algorithm to test the
framework's ability to find the true model
parameters. \emph{GillespieSSA} provides an easy to use, extensible
means of generating simulated trajectories of finite population
continuous-time models. Algorithm 1 shows a basic implementation of the
SIR model in R using \emph{GillespieSSA} for generation of synthetic data.

\begin{algorithm}
  \captionof{figure}{Implementation of the SIR model using the GillespieSSA package}
  \begin{algorithmic}
    \Function{gillespie.ssa.sir}{$params, I0, time, i$}
    \State \emph{\# Define parameters}
    \State parms $\gets$ c(beta=params[1],gamma=params[2])
    \State
    \State \emph{\# Define system}
    \State x0 $\gets$ c(S=params[3], I=I0, R=0) \Comment{Initial state vector}
    \State nu $\gets$ matrix(c(-1,0,1,-1,0,1),nrow=3,byrow=T)
    \Comment{State-change matrix}
    \State a  $\gets$ c("beta*S*I", "gamma*I")  \Comment{Propensity vector}
    \State tf $\gets$ time \Comment{Final time}
    \State
    \State \emph{\# Run the simulations}
    \State nf $\gets$ layout(matrix(c(1,2,3,4),ncol=2,byrow=T))
    \State
    \State \emph{\# Direct method}
    \State set.seed(i)
    \State out $\gets$ ssa(x0,a,nu,parms,tf,method="ETL",tau=1,
    simName,verbose=FALSE)
    \State return out.data
\EndFunction
\label{algorithm:gillespie}      
 
\end{algorithmic}
\end{algorithm}


\section{Solving Candidate Models}
As we aim to find a model that best describes our data, the next
consideration is the generation of model data that might fit our
epidemic data. With a candidate set of model equations in mind,
namely the \emph{SIR} model, and a set of candidate parameters (beta,
gamma and S0), we solve the model ODEs to generate a series of
discreate data. We can then assess how well this chosen model fits the
epidemic data.

When implementing the model fitting
framework in \emph{R}, we utilise the \emph{ode} function from the
\emph{deSolve} package to return sub-population values calculated from a
given set of parameters, a set of ODEs and a desired time frame. A
simple \emph{R} implementation of the \emph{SIR} is shown in
Figure~\ref{fig:sirR}. In the C++ implementation, an ODE solving
framework is written from scratch.

\begin{algorithm}
\label{fig:sirR}
\captionof{figure}{Implementation of the Kermack-McKendrick SIR model}
\begin{algorithmic}
  \Function{closed.sir.model}{$time, data, parameters$}
  \State S $\gets$ data[1]
  \State I $\gets$ data[2]
  \State R $\gets$ data[3]
  \State
  \State beta $\gets$ parameters[1]
  \State gamma $\gets$ parameters[2]
  \State
  \State dS $\gets$ -beta*S*I
  \State dI $\gets$ beta*S*I - gamma*I
  \State dR $\gets$ gamma*I
  \State
  \State list(c(dS,dI,dR))
  \EndFunction
\end{algorithmic}
\end{algorithm}
  
  
With a model solving framework and a set of data that we wish to fit,
one can begin to visualise how the model fitting process might take
place. Even with completely unknown parameters, candidate models can
be generated by choosing parameters that might fit the
data. Figure~\ref{fig:sircurves} depicts how using various model
parameters results in different
shaped curves. 

\begin{center}
\begin{figure}[ht!]

\includegraphics[width=15cm]{Rplot01.png}
\caption{Graph demonstrating various levels of model fit}
\label{fig:sircurves}
\end{figure}  
\end{center}

As a theoretical aside, it should be highlighted that the
\emph{ode} function uses the LSODA integration method by default,
based on FORTRAN code. The benefit of the LSODA method is that is
automatically switches between stiff and non-stiff systems, and is
very robust. However, in our C++ implementation, we provide a `from
scratch' ODE solver using the Runge-Kutta method in an attempt to
speed up the model fitting process. A theoretical introduction to ODE
solving is provided in Box 4.  

\newpage
\begin{framed}
{\begin{center}{\bf Box 4: Solving Ordinary Differential Equations}\end{center}}

The epidemic models take the form of ordinary differential
equations (ODEs), wherein the dynamics of the population are described by the
transition of individuals between different compartments. In the case
of the the \emph{SIR} model, individuals transition from susceptible,
to infected and finally to recovered. As discussed in Box 1, the rate
of transition between these states depends on the model parameters
(namely beta and gamma), as well as the number of individuals
currently in each compartment. 

For the purpose of model fitting, it is necessary to calculate the
number of individuals in each compartment at each time point. This
requires the set of ODEs to be solved. Given a set of parameters and
initial compartment sizes, we wish to find the number of individuals
over the course of the epidemic at each time point. For a simple
differential equation, it is possible to find the closed form
solutions. Given a function, \emph{g}, we wish to find the solution
such that:

\begin{equation}
\begin{split}
  &Y'(t) = g(t)\\
  &Y(t) = \int g(s)ds+c
\end{split}
\end{equation}

where \emph{c} is an arbitrary constant, and the value of Y(t) can be
obtained at a given time point: \begin{equation} Y(t_0) = Y_0 \end{equation}

In the case of
first-order differential equations (as is our case), we take the above
equation as the initial value condition and are presented
with an initial value problem of the form:\cite{atkinson} 

\begin{equation}
\begin{split}
  &y'(t) = f(t,y(t)),\\
  &y(t_0) = y0
 \end{split}
 \end{equation}

It is often impractical to derive analytical solutions to
differential equations. In the case of models in epidemiology, the
first-order different equations are often
non-integrable.\cite{shabbir} Considerable work has been undertaken to attempt to solve
\emph{SIR} models analytically using Lie analysis and homotopy
analysis.\cite{nucci, khan} Such approaches are difficult to implement
and are not fit for the purpose quickly solving ODEs in a
generalisable way. We therefore turn to numerical
analysis.

Numerical methods are used to find numerical approximations to the
solutions of ODEs rather than solving them analytically. For the
purpose of obtaining population values that can be used in model fit
assessment, such numeric approximations are sufficient. The simplest
numerical method for solving the initial value problem is \emph
{Euler's method}, which involves finding an approximate nearby point
on the curve by moving along a line tangent. Euler's method forms
the basis for a highly popular group of
methods for solving initial value problems known as Runge-Kutta
methods, which are relatively easy to implement. 

The most basic member of the Runge-Kutta methods is simply known as
the ``classical Runge-Kutta method'', and is the method that we chose
to implement in C++. Given the above initial value problem and an
initial condition, we can attempt to find later values of y(t) with
the following definitions:

\begin{equation}
\begin{split}
  &y_{n+1} = y_n + \frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\\
  &t_{n+1} = t_n + h
 \end{split}
\end{equation}
for \emph{n} = 0, 1, 2, 3..., using
\begin{equation}
\begin{split}
  &k_1 = f(t_n,y_n),\\
  &k_2 = f(t_n + \frac{h}{2},y_n+ \frac{h}{2}k_1),\\
  &k_3 = f(t_n + \frac{h}{2},y_n+ \frac{h}{2}k_1),\\
  &k_4 = f(t_n+h,y_n+hk_3)
\end{split}
\end{equation}

Note that $y_{n+1}$ is the Runge-Kutta approximation of $y(t_{n+1})$,
where $y_{n+1}$ is determined by the weighted average of four
increments of $y_n$ at interval size, \emph{h}, with the estimated
slope specified by the right hand side of the differential
equations. Note that greater weighting is given to the increments at
the midpoint of the chosen interval.
\end{framed}

\section{Parameter Optimisation}
We now have now clearly identified our problem and the
means by which we can attempt to solve it. That is, can we find a set
of ODE parameters that generate a model that accurately fits our epidemic
data. Given an initial set of test parameters, we attempt to optimise
these parameters to best fit our data using an optimisation algorithm
alongside an objective function measuring model fit.

\subsection{Initial Test Parameters}
In classical epidemiology, it is often possible to obtain estimates of
many important model parameters based on disease biology and
population dynamics. For example, the initial susceptible population
of an isolated influenza outbreak might be estimated as the school-aged
population of a country. Similarly, the infection and recovery rates of a
new strain of virus might be estimated based on phylogenetic relationships
to previous viruses of known parameters.\cite{volz} Another potential method is to measure
transmission rates in experimental populations, as demonstrated by
Bouma et al., who estimated the transmission parameters of the H5N1
avian influenza virus using a small number of birds in an experimental
tranmission study.\cite{bouma} However, it is easy to imagine
situations where parameter estimation might be infeasible. Estimating
the susceptible population size for a viral \emph{YouTube} video, for
example, might be difficult. Do we assume that the entire internet
population is at risk of exposure, or will the video be limited to only
certain internet communities? Such a scenario is not unimaginable for
infectious diseases. Should a new, uncharacterised disease arise in
only an unknown demographic, the task of estimating model parameters
becomes very difficult.

In scenarios where parameter estimation is infeasible, we aim to find
the true model parameters without making any assumptions as to where
they might lie other than within a realistic range. In the presented
model fitting framework, we begin the parameter optimisation procedure
with random parameter values taken from a realistic range with
reasonable limitations imposed. For example, we seed the optimisation
procedure with a random beta value between 0.0001 and 0.01. It is also
important to ensure that a number of realistic conditions are adhered
to:

\begin{enumerate}
  \item The basic reproductive ratio must be sufficient to allow an
    epidemic to take off, $R0 > 1$. For this to be adhered to, gamma
    must be greater than beta.
  \item The initial number of susceptible individuals, $S_0$ must be
    positive and within a reasonable range. Seeding with an
    very high or low values might prevent the optimisation
    procedure from converging on an optimal solution.
  \item The initial number of infected individuals, $I_0$, must be
    greater than 0. Whilst $I_0$ does not necessarily need to be bound
   from above by $S_0$, it is generally the case that $S_0$ is
   much greater than $I_0$. 
\end{enumerate}

One heuristic for estimating the start value of $I_0$ is to take the
first data point as the initial number of infected
individuals. However, this causes the model to be highly dependent on
the first data point, neglecting to consider that the first data point
might not represent the start of the epidemic. A more reasonable
approach would be to consider $I_0$ as another unknown parameter to be
optimised, or to assume that there is initially only one infected
patient, or `patient zero'. In the case of online phenomena, an $I_0$
of 1 may represent the initial posting of a video or meme. In the case
of infectious disease dynamics, $I_0$ might need to be seeded
higher. For example, multiple infecteds might enter a population
simultaneously on the same flight. We initially make the assumption
that $I_0$ is always 1, and then go on to adapt our
implementation to include $I_0$ as an unknown parameter.


\subsection{The Objective Function}
With a set of model equations, potential model parameters and the
resulting model data at each time point, the next step is to assess
how well the proposed model fits the given data. It is only through
quantifying this measurement that we can then go on to find the best
fitting model parameters. As discussed in section BACKGROUND, the
first assessment of fit that we implement is the least squares
fit. This uses the total squared difference between each model
value and dataset value at each time point. The smaller this `sum of
squared errors' (SSE), the better the model fit. Clearly our aim is to find
the set of parameters that minimises this SSE. A central part of the
model fitting framework is therefore the implementation of this
`objective function' (Figure~\ref{algorithm:objective}). Once the objective function is
defined, the final step in the optimisation procedure is to transform
the model parameters until the SSE is minimised.

\begin{algorithm}
 \captionof{figure}{Least Squares Fitting Objective Function}
\begin{algorithmic}
  
  \State \# Takes a set of parameters and a set of
  epidemic data. The \emph{ode} function then uses the LSODA solver to
  evaluate the \emph{SIR} model. The sum of squared
  errors is then calculated from the generated model and provided data.
  \State
  \Function{sir.sse}{$params, data$}

  \State t $\gets$ data[,1]
  \State cases $\gets$ data[,2]
  \State
  \State beta $\gets$ params[1]
  \State gamma $\gets$ params[2]
  \State
  \State S0 $\gets$ params[3]
  \State I0 $\gets$ 1
  \State R0 $\gets$ 0
  \State
  \State out $\gets$ as.data.frame(ode(y=c(S=S0,I=I0,R=R0),
  times=t,closed.sir.model,parms=c(beta,gamma),
  atol=1e-15,hmax=1/120))
  \State sse $\gets$ sum((out\$I-cases)\^2)
  \EndFunction
 \end{algorithmic}
\label{algorithm:objective}
\end{algorithm}


\subsection{Optimisation Algorithm}
The next step in the model fitting framework is an implementation of
an optimisation procedure to find the set of parameters that minimise
the result of the objective function. In the initial \emph{R}
implementation, this is done by passing the initial seed parameters,
the objective function and the data to the \emph{optim}
function. \emph{Optim} uses the Nelder-Mead algorithm to find the set
of parameters in the parameter space that return the minimum objective
function value. That is, the set of parameters that evaluate to a
model that most closely fits the provided data. Box 4 provides a
theoretical overview of the Nelder-Mead algorithm. 

The \emph{optim} function also provides the option to use other optimisation methods,
including the ``BFGS'' quasi-Newton method, the ``CG'' conjugate
gradients method and the ``L-BFGS-B'' method. However, we settle on the
Nelder-Mead due to its robustness, and in the case of C++, its ease of
implementation. 

Figure~\ref{fig:simple}  illustrates the results of running \emph{optim} on a
\emph{GillespieSSA} generated model with known parameters.

\begin{centering}
\begin{figure}
\includegraphics[width=15cm]{simplefit.png}
\caption{The Gillespie algorithm is run with original parameter values of $\beta =
  0.001, \gamma = 0.1, S_0 = 500$. The \emph{optim} function returns
  fitted parameter values of $\beta = 0.0008018066, \gamma = 0.1273486, S_0
  = 618$. This results in a SSE of 5120.06.}
\label{fig:simple}
\end{figure}
\end{centering}



\newpage
\begin{framed}
{\begin{center}{\bf Box 5: The Nelder-Mead Algorithm}\end{center}}
The Nelder-Mead algorithm, or simplex search algorithm, is one of the
best known and commonly used algorithms for multidimensional
unconstrained optimisation without derivatives. The algorithm is
relatively simple to understand and implement, which makes it an ideal
candidate for solving parameter estimation problems. The method
ultimately approximates a local optimum of a problem with $N$
variables when provided with an objective function to be minimised.

Given a nonlinear function, $f : {\mathbb
  R}^n \to {\mathbb R}\ .$, the Nelder-Mead algorithm uses a
simplex-based search method to minimise $f$, where a simplex, $S$
in ${\mathbb R}^n$ is defined as the convex hull of $n + 1$ vertices,
$x_0,...,x_n \in {\mathbb R}^n$. In the case of ${\mathbb R}^2$, the
simplex is a triangle, whereas in the case of ${\mathbb R}^3$, the
simplex is a tetrahedron. In the case of epidemic model fitting, each
vertex of the simplex corresponds to a set of model parameters.

Starting with a set of $n+1$ points (the initial `seed' parameters)
and a corresponding set of function values at the vertices, $f_i :=
f(x_i),$ for $j = 0,...,n$, the Nelder-Mead method performs a sequence
of transformations on the working simplex S with the aim of decreasing
the function values at its vertices. Once the method has satisfied
some minimisation condition, whether it be a number of steps or a
desired minimisation range, the final simplex can be used to return
the optimised parameters.

The Nelder-Mead algorithm follows the following set of
steps:

\begin{enumerate}
\item Construction of the initial working simplex, $S$, around an initial
  point based on initial parameters
\item Repeat the following steps until maximum number of iterations
  reached or minimisation condition satisfied:
  \begin{enumerate}
  \item Calculate the function value for the current working simplex
  \item Check if terminiation criteria met
  \item If not met, transform towards the best vertex of the working simplex to give new
    vertex values with the following sub steps:
    \begin{enumerate}
      \item Determine the order of vertices in terms of function
        values (from best to worst)
        \item Calculate the centroid, $c$ of the side opposite the
          worst vertex
\item Compute a new working simplex from the current simplex using a
  series of transformations
\end{enumerate}
  \end{enumerate}
\item Return the best vertex of the current simplex, S, along with
  its associated function value.
\end{enumerate}

In the transformation step, replacing the worst vertex is achieved by reflection, expansion or contraction with respect to the best
side. Firstly, the worst vertex is replaced with a reflection of the
best vertex. If this point provides an improvement on the current best
value, then the simplex is expanded towards the new point. Otherwise,
the simplex is contracted towards the current best point.If
successful, then the new point replaces the worst vertex of the
working simplex. If not, then the simplex is shrunk towards the best
vertex. A later addition of the algorithm is to shrink the entire
simplex in the event of failed contractions, though this is a rare and
slow step.

\begin{center}
  \includegraphics[width=15cm]{nelder.png}
  \captionof{figure}{A) Reflection. B) Exansion. C) Outside
    contraction. D) Inside contraction. E) Shrink transformation}
\end{center}

The Nelder-Mead method is a fast and relatively simple
algorithm for obtaining a good reduction in function value in a
relatively small number of function evaluations. For optimisation
problems where a precise optimum (eg. parameters are subject to noise)
is not necessarily required, the Nelder-Mead method is ideal. However,
it i s possible for the
Nelder-Mead algorithm to undertake an extremely high number of
iterations with little to no function value improvement at a region
far from the actual minimum. A heuristic solution to this problem is
to restart the algorithm at multiple start points and to only allow a
small number of iterations during each run.\cite{nelder,singer}
 
\end{framed}


\subsection{Parameter Transformations and Bounding}
When left unmodified, the Nelder-Mead algorithm will search the parameter
space in the range of $-\infty$ to $+\infty$. However, in the context
of epidemic modelling, it does not make sense to consider negative
parameters. By doing so, we increase the chance of the algorithm
getting stuck in a local minimum and therefore returning infeasible results. We therefore carry out a \emph{log transformation} on each
parameter, only carrying out the reverse \emph{exp} transformation
within the objective function. This constrains the search space to
between $0$ and $+\infty$, ensuring that we do not generate
meaningless parameter vectors. 

Another potential transformation that may be applied is through the
use of the \emph{logistic} function. Originally studied in the context
of population growth, maps the parameter search space from between $0$ and $1$
to between $-\infty$ and $+\infty$. The function is
defined as:
\begin{equation}
logistic(x)=\frac{1}{1+e^{-x}}
\end{equation}
The inverse of the logistic function, the \emph{logit} function, can
therefore be used to map the search space of a parameter, \emph{p},
from between $-\infty$ and $+\infty$ to between $0$ and $1$. The logit
function is defined as:
\begin{equation}
logit(p) = log(\frac{p}{1-p})
\end{equation}
The logit function can be modifed to transform the search space from
between $0$ and $1$ to one between $0$ and $max$ as follows:
\begin{equation}
newLogit(x) = \frac{max}{1+e^{-x}}
\end{equation}
with the inverse defined as:
\begin{equation}
newLogistic(x)=log(\frac{x}{max-x})
\end{equation}

The logic behind this modification can be expanded further to use the
logistic function to provide lower bounds as well as upper bounds. As
$x\rightarrow +\infty$, $logit(x)\rightarrow 1$. Similarly, as
$x\rightarrow-\infty$, $logit(x)\rightarrow 0$. We can therefore modify
the logistic function as follows:
\begin{equation}
f(x) = \frac{x_{max}-x_{min}}{1+e^{-x}} + x_{min}
\end{equation}.

We can see that as $x\rightarrow +\infty$, $f(x)\rightarrow x_{max}$,
and as $x\rightarrow -\infty$, $f(x)\rightarrow x_{min}$. By limiting
the parameter search space to a predetermined range of expected
values, we ensure that the optimisation procedure does not get stuck
in a local minimum far away from the actual parameter values. However,
doing so requires us to make assumptions regarding where the real
parameter values might lie. Whilst a good model fit might be produced
within a provided range of parameter values, this fit might be sub
optimal compared to the model produced from a set of entirely
unexpected parameters. 

An alternative, simple approach to bounding parameter values in the
optimisation procedure is to modify the results returned by the
objective function. As the Nelder Mead algorithm transforms parameter
values with the aim of minimising the objective function, we can
direct the parameter search by ensuring that the objective function
returns high values when venturing into an undesirable parameter
space. For example, we can include a simple check of each parameter
with each call of the objective function; returning a high SSE value
if the parameter is outside our desired range. This method was
implemented initially; however, it appeared to result in the Nelder
Mead algorithm returning nonsense values more frequently as it failed
to identify any local minima. This is due to the fact that the
optimisation surface becomes much less smooth, as each check outside
of the specified bounds results in a sudden spike in function value.  


\section{Evaluating Goodness of Fit}
Although the SSE provides a value which the optimisation process can
aim to minimise, it's magnitude is largely meaningless without
appropriate context. We therefore use the coefficient of
determination, $R^2$, to provide an evaluation of model fit. $R^2$ is
a widely used measure of goodness of fit in statistical modelling, and
provides an ideal means for comparison between different model
fits. $R^2$ provides a measure of what proportion of total variation
in the data is explained by the model. A mathematical definition of
the $R^2$ measure is provided in Box 5.

\newpage
\begin{framed}
{\begin{center}{\bf Box 6}\end{center}}
{\bf The Coefficient of Determiniation}:\\

Consider a data set with observed values $y_i$. We would like to assess how
well a set of predicted values $f_i$ fits our data. Firstly, we
consider the amount of variability in our data set using the sum of
squares:

\begin{equation}
  SS_{tot} = \sum\limits_{i}(y_i - \bar{y})^2
\end{equation}

Where $\bar{y}$ denotes the mean of the observed:

\begin{equation}
  \bar{y} = \frac{1}{n}\sum\limits_{i=1}^n(y_i)
\end{equation}

Next, we calculate the sum of squares of the residuals. That is, the
amount of discrepancy between the data and the predicted values:

\begin{equation}
SS_{res} = \sum\limits_{i}(y_i - f_i)^2
\end{equation}

The final step is then to evaluate the amount of unexplained variance of
the model with the total variance of the data. This gives us the
\emph{coefficient of determination}, $R^2$:

\begin{equation}
  R^2 \equiv 1 - \frac{SS_{res}}{SS_{tot}}
\end{equation}

The resulting value is usually a number between 0 and 1, where 1
suggests that the predicted values explain all of the variance of the
data (a perfect fit), and a value close to 0 implies that it explains
very little of the data
variance (a poor fit). Values of less than 0 are also possible, which indicates
that the mean of the data provides a better model fit than the model.

\end{framed}

\section{Iterative Least Squares Fitting Framework}
\subsection{R Implementation}
Due to the availibility of relevant packages and its suitability for
data manipulation, the initial fitting framework was implemented in
R. The flow of control of the basic implementation is depicted in
Figure~\ref{process}. The framework takes a matrix or data frame of epidemic data
to be fit and begins the optimisation procedure. Random seed
parameters are generated and given to the optimisation function,
\emph{optim}, along with the objective function to be minimised. As
the Nelder-Mead algorithm can converge on sub-optimal solutions due to
the presence of local minima, the optimisation function is restarted
twenty times with different seed parameters. The best fitting set of
parameters are stored and passed to the analysis procedure, which
evaluates the model for the given parameters and calculates the
accompanying R-Square value. Finally, all of the results are passed to
the output procedure for graph plotting and results saving.  

\begin{centering}
\begin{figure}[ht!]
  \includegraphics[width=15cm]{images/process.png}
 \caption{Control flow of the basic model fitting framework} 
\label{fig:process}
\end{figure}
\end{centering}

To begin with, the optimisation procedure only considers beta and
gamma to be unknown. The other important model parameters, S0, I0 and
R0 are set to the correct values. We then adapt the framework to also
consider S0 to be a completely unknown parameter. Figure~\ref{fig:sir} shows how the optimisation
procedure unfolds over time. The framework fits the available data
points well early on; however, it does not accurately predict the main
peak until sufficient data has become available. By the 40th data
point, a very high R-Square value is achieved, with parameters that
closely match the ground truth parameters (beta=0.00111, gamma=0.0946,
S0=506). The inclusion of S0 as an additional unknown parameter allows
a closer model fit to be calculated.

\begin{centering}
\begin{figure}[h!]
  \includegraphics[width=8cm]{images/sirs0_5.jpeg}
  \includegraphics[width=8cm]{images/sirs0_10.jpeg}
  \includegraphics[width=8cm]{images/sirs0_18.jpeg}
  \includegraphics[width=8cm]{images/sirs0_40.jpeg}
\caption{Iterative model fitting over time using least squares
  minimisation.}
\label{fig:sir}
  \end{figure}
\end{centering}

The model fitting procedure can be extended further to include the
number of initial infected individuals, I0. Data from real epidemic
phenomena might not become available until well into the start of the epidemic, and number of initial infected individuals might not immediately
be available. For example, consider a \emph{YouTube} video where views
data are not collected until the video has already become viral. By
including I0 in the optimisation procedure, we
are able to predict the dynamics of the epidemic as soon as it is
detected. Including additional unknown parameters in the
optimisation procedure provides a closer model fit but runs the risk
of increase instability during optimisation as parameter
transformations have a greater impact on the objective
function. Figure~\ref{fig:siri0} shows the run of the optimisation procedure where
beta, gamma, S0 and I0 are all assumed to be unknown. Although the
fits seem comparable to Figure~\ref{fig:sir}, Figure~\ref{fig:siri0wrong} demonstrates how the
goodness of model fit is reduced when our initial assumptions about I0
are incorrect.

\begin{centering}
\begin{figure}
\includegraphics[width=8cm]{images/siri0_5}
\includegraphics[width=8cm]{images/siri0_10}
\includegraphics[width=8cm]{images/siri0_20}
\includegraphics[width=8cm]{images/siri0_40}
\caption{Iterative model fitting over time using least squares
  minimisation. I0, S0, beta and gamma unknown.}
\label{fig:siri0}
\end{figure}
\end{centering}


\begin{figure}
  \centering
\includegraphics[width=10cm]{images/siri0_unknown}
\caption{Goodness of fit reduced when inaccurate parameter assumptions
  are made.}

\label{fig:siri0wrong}
\end{figure}

\subsection{C++ Implementation}
Although the implementation in R provides a simple and relatively quick
fitting framework, it depends heavily on independently provided
packages. By implementing the fitting framework from scratch, we are
able to address any potential bottlenecks to performance. Furthermore,
it has been found that C++ is much faster than R in modelling
tasks.\cite{languagespeeds} On the flip
side, the lack of available methods imposes a significant cost in
terms of coding time on the project. A significant contribution of this project
is the implementation of model fitting framework from scratch using a
generalisable, objected oriented approach, allowing for the easy
addition of candidate epidemic models. Figure~\ref{fig:uml}  depicts a simplified
UML diagram of the object oriented model fitting framework.


\begin{figure}
  \centering
 \includegraphics[width=15cm]{images/uml}
\caption{UML diagram of the model fitting framework}
\label{fig:uml}
\end{figure}

The Datahandler class keeps track of the currently available data,
best fitting parameters, current model and currently active
epidemics. The fitting procedure is initiated with a desired fit
level, and the Datahandler proceeds to manage the overall fitting
process. An objective function is formed by solving and summing the
ODEs for each active epidemic, and calculating the SSE against the
available epidemic data. This objective function is passed to the
Simplex class, which carries out the Nelder-Mead algorithm to return
an optimised set of parameters. Note that the Epidemic class is an
abstract class, with each sub class having its own set of parameters
and set of ODEs. The Epidemic class also holds the Runge Kutta method
for solving a given set of ODEs. This allows the framework to be easily extendible to
include additional types of candidate model. The Datahandler class
also manages the analysis of model fit and the plotting of any desired
graphs through the use of Gnuplot. Figure~\ref{fig:sirc} shows the output of the
iterative fitting procedure on synthetic data, where beta, gamma and
S0 are assumed to be unknown.

\begin{figure}
  \centering
  \includegraphics[width=8cm]{images/output5}
  \includegraphics[width=8cm]{images/output10}
  \includegraphics[width=8cm]{images/output20}
  \includegraphics[width=8cm]{images/output40}
\caption{Graphical output of the C++ single model fitting
  framework. Beta = 0.001, gamma = 0.1, S0 = 1000, I0 = 1}
\label{fig:sirc}
\end{figure}


\section{Alernative Candidate Models}

The generalised C++ implementation allows for alternative candidate
models to be fit to our epidemic data. In classical epidemiology, many
infectious diseases are better explained by adaptations of the
\emph{SIR} model. For example, The HIV virus is best explained by the
\emph{SIS} model, where infected individuals do not recover but rather
return to the susceptible compartment.\cite{vynnycky} Similarly, some online epidemic
phenomena have been shown to be better described by an adaptation of
the \emph{SIR} model known as the \emph{irSIR} model, which suggests
that the recovery rate is additionally dependent on the number of
recovered individuals (as themes become `out of
fashion').\cite{cannarella} As discussed in section BACKGROUND, it has
also recently been proposed that online viral trends might spread as either gradual
``growth'' or sudden ``spike'' epidemics, depending on whether the
trend is spread slowly through social networks, or rapidly through
mass exposure. An improved model fitting framework would
therefore allow for the selection of a best fitting candidate model
from a list of potential models.

\subsection{Additional Models}
The present fitting framework considers the SIR and SEIR as initially described
in classical epidemiology.\cite{vynnycky} We also include the Exponential Decay model
to represent a ``spike'' epidemic and the irSIR mdoel as proposed by
Cannarella et al.\cite{cannarella} Furthermore, we propose a novel
adaptation of the SEIR model named the SErIR model. In this model, we
consider the number of exposed individuals rather than the number of
infected individuals as our population of interest. The rationale
behind this is that in
online phenomena, an individual is recorded as having seen or viewed a
video regardless of whether or not they are actively spreading the
epidemic. Furthermore, this model adds an additional parameters,
$\pi$, which represents the rate at which individuals who are exposed
to the `infection' move to the recovered compartment without spreading the infection.

\begin{figure}
\centering
\includegraphics[width=16cm]{images/models}
\caption{1) SIR model. 2) SEIR model. 3) Exponential decay model. 4)
  Modified SErIR model. 5) irSIR model}
\end{figure}

\newpage
\begin{framed}
{\begin{center}{\bf Box 5: Candidate Epidemic Models}\end{center}}
In addition to the \emph{SIR} and \emph{Exponential Decay Model}
described in Box 2, we include the following epidemic models in our
fitting framework:

{\bf The irSIR Model}\\
Originally proposed by Cannarella et al., the \emph{irSIR}
model is an adaptation of the \emph{SIR} model to consider that the
rate of recovery depends on the number of recovered
individuals. The classic \emph{SIR} model considers that infected
individuals recover with a set rate, $\gamma$ as they fight off the
infection. The idea behind the irSIR model is that an individual is
more likely to recover from an infection if they come into contact
with an already recovered individual. For example, Cannarella et
al. suggest that individuals are more likely to stop using a social
network site as overall usage decreases. The \emph{irSIR} model has the following parameter vector and set of
equations:
\begin{equation}
\centering \theta^{(i)} = [I_0^{(i)},
  S_0^{(i)}, \beta^{(i)}, \gamma^{(i)}]
\end{equation}
\begin{centering}
\begin{equation}
	\begin{split}
	&\frac{dS}{dt} = -\beta IS, \\
	&\frac{dI}{dt} = \beta IS - \gamma IR, \\
	&\frac{dR}{dt} = \gamma IR
	\end{split}
\end{equation}
\end{centering}\\
{\bf The SEIR Model}\\
The \emph{SEIR} is another simple adaptation of the \emph{SIR} model
that aims to better describe the course of a disease. Rather than
transitioning from susceptible to infected immediately, many diseases
go through a long incubation period before the infected individual
becomes infectious.\cite{aron} This incubation period is described by
the parameter $\alpha$, where $1/\alpha$ is the mean latent period of
the disease. The \emph{SEIR} model has the following parameter vector
and set of equations:
\begin{equation}\centering\theta^{(i)} = [I_0^{(i)},
  S_0^{(i)}, \beta^{(i)}, \alpha^{(i)}, \gamma^{(i)}]\end{equation} 
\newpage
\begin{centering}
\begin{equation}
	\begin{split}
	&\frac{dS}{dt} = -\beta IS, \\
        &\frac{dE}{dt} = \beta IS - \alpha E, \\
	&\frac{dI}{dt} = \alpha E - \gamma IR, \\
	&\frac{dR}{dt} = \gamma IR
	\end{split}
\end{equation}
\end{centering}\\
{\bf The SErIR Model}\\
A novel candidate model proposed here is the \emph{SErIR} model; an
adaptation of the SEIR model that takes into account incomplete infection
spreading. Whereas individuals affected by an infectious disease will
invariably spread the infection upon contact, individuals that are
exposed to online viral phenomena might not go on to become
spreaders. Furthermore, the measure interest becomes the `exposed'
rather than `infected' compartment, as non-spreaders will contribute
towards the data of interest. Consider the example where an individual
views a \emph{YouTube} video, but does not go on to `share' the link
with anyone else. We introduce the parameter, $\pi$ to describe the
rate at which individuals move from the exposed to the recovered
compartment, where $1/\pi$ might be termed the ``ignoral rate''. The
\emph{SErIR} model has the following parameter vector and set of
equations:
\begin{equation}\centering\theta^{(i)} = [I_0^{(i)},
  S_0^{(i)}, \beta^{(i)}, \alpha^{(i)}, \pi^{(i)}, \gamma^{(i)}]\end{equation} 
\begin{centering}
\begin{equation}
	\begin{split}
	&\frac{dS}{dt} = -\beta IS, \\
        &\frac{dE}{dt} = \beta IS - (\alpha + \pi)E, \\
	&\frac{dI}{dt} = \alpha E - \gamma IR, \\
	&\frac{dR}{dt} = \gamma IR + \pi E
	\end{split}
\end{equation}
\end{centering}

\end{framed}

Figure~\ref{figure:candidates} shows the trajectories of the various
candidate models when seeded with similar parameters. As individuals
move out of the compartment of interest at a greater rate (namely in
the \emph{irSIR} model), the peak becomes much lower. Furthermore, the
introduction of an incubation period as in the \emph{SEIR} and
\emph{SErIR} models delays the peak of the epidemic.

\begin{figure}
\centering
\includegraphics[width=16cm]{images/candidates}
\caption{Dynamics of candidate models with similar parameters and
  starting susceptible population of 1000}
\label{figure:candidates}
\end{figure}

\subsection{Selecting a Model}
The implementation described here allows the user to either fit a
specified epidemic type to the set of data. All relevant
transition rate parameters (eg. beta, gamma) and S0 are included in the
optimisation process, and we allow the user to optionally include I0
as an unknown parameter. The user may also decide to allow the fitting
framework to select the best fitting model for the data. In this case,
the framework attempts to fit each candidate model and tracks the best
fitting result, settling on the candidate model that best describes
the data. This process is not as simple as choosing the model
with the best R Square value. The shape of models with more parameters
is much more adaptable by nature, as the increased number of
parameters results in more inflection points. As all of the candidate
models are extensions of the \emph{SIR} model, the more complex models
(\emph{SEIR}, \emph{SErIR}) will tend to be selected simply due to
their increased complexity. To prevent this overfitting, the relative
model complexity must also be taken into consideration when selecting the best candidate model.

The aim of the candidate model selection procedure is therefore to
select the simplest model that provides a good fit to the data. A
heuristic approach to this problem is to select the model with the
fewest parameters that provides a sufficiently good fit. When
considering which model to add, we first check the R Square value for
each model type and only consider those models that provide a fit
above a pre specified threshold. If more than one model provides a
sufficiently good fit, we choose the model with the fewest
parameters. If two or more models have the same number of parameters
and provide a sufficiently good fit, then we are able to choose the
with the highest R Square value. 

A formal measure of the above heuristic is known as the \emph{Akaike
  information criterion}, or \emph{AIC}.\cite{burnham} Based on information theory, the \emph{AIC} is a measure
of statistical model quality for a given set of data that takes into
account both goodness of fit and model complexity. In general terms, the \emph{AIC}
value of a model is defined as:
\begin{equation}
AIC = 2k - 2ln(L)
\end{equation}
where $k$ is the number of free model parameters, and L is the maximised
likelihood function. The preferred candidate model is that with the
minimum \emph{AIC} value, which therefore minimises information loss. As we are only interested in the comparative
\emph{AIC} values, we can use the $\delta$\emph{AIC} value between two
models. Furthermore, we can also make adjustments for small sample
sizes, as is the case with our epidemic data early on in the fitting
procedure. This \emph{AICc} puts a greater penalty on complex models
than the basic \emph{AIC} score, and is defined as:
\begin{equation}
AICc = -2ln(L) + 2k + \frac{2k(k+1)}{n - k - 1}
\end{equation}
In the scenario where we use the residual sum of squares as our
objective function value, we use the following \emph{AIC} definition:
\begin{equation}
AIC = nln(RSS/n) + 2k
\end{equation}

where $n$ is the number of data points and $RSS$ is the residual sum
of squares.

In terms of implementation, we calculate the \emph{AICc} value for
each candidate model and track the candidate model with the lowest
\emph{AICc} value. Once all candidate models have been considered, the
final model with the lowest \emph{AICc} value is chosen as the best
fitting candidate model. As the first window of data points might not
be representative of the entire data set, we perform this selection
every 10 data points to ensure that the best model is chosen in light
of recent data. 

\section{Maximum Likelihood Based Estimation}
An alternative to least squares based fitting is the use of Maximum
Likelihood Estimation (MLE) as an objective function. The theory  ,
